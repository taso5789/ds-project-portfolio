[
  {
    "objectID": "10_conclusions.html",
    "href": "10_conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "xxx\nxxx\nxxx\nxxx\nxxx"
  },
  {
    "objectID": "08_regression.html",
    "href": "08_regression.html",
    "title": "Regression",
    "section": "",
    "text": "Overview\n\nxxx\n\n\n\nData Prep\n\nxxx\n\n\n\nCode\n\nxxx\n\n\n\nResults\n\nxxx\n\n\n\nConclusions\n\nxxx"
  },
  {
    "objectID": "06_decision_trees.html",
    "href": "06_decision_trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "A Decision Tree is a supervised learning model that uses a tree-like structure similar to a flowchart to classify or regress data. It arrives at a final prediction by repeatedly asking a series of “if-then-else” questions. A major advantage of this model is that its results are very easy to interpret due to its intuitive structure.\nA decision tree is composed of the following elements:\n\nRoot Node: The starting point of the tree, representing the first question.\nDecision Node: An intermediate node that represents an intermediate question used to further split the data.\nLeaf Node (or Terminal Node): The endpoint of a branch, representing the final predicted outcome (a class label or a numerical value).\n\nThis model is utilized in a wide range of fields, including customer purchase prediction, medical diagnosis, and credit scoring.\n\n\n\n\n\n\n\n\nA visual comparison of machine learning models. Supervised learning (left), which includes algorithms like Decision Trees, uses labeled data to classify a new point. In contrast, unsupervised learning (right) finds hidden clusters in unlabeled data. Source: Wikimedia Commons\n\n\n\n\n\n\n\n\nA decision tree model predicting Titanic passenger survival. The structure flows from the initial root node (‘gender’), through intermediate decision nodes (‘age,’ ‘sibsp’), to the final predictions in the leaf nodes (‘survived’/‘died’). Source: Wikimedia Commons\n\n\n\n\n\n\n\n\n\n\nAt each step, a decision tree must determine “which question will most efficiently split the data.” To make this judgment, it uses metrics like Gini Impurity or Entropy. These metrics measure the “impurity” or “disorder” of the data within a node.\nGini Impurity: This metric indicates how mixed the classes are within a node. A value close to 0 means the node is “pure” (contains data from mostly one class), while a higher value means it is “impure” (contains a mix of different classes). It is known for being relatively fast to compute. \\[\nGini(S) = 1 - \\sum_{i=1}^{c} (p_i)^2\n\\] where \\(p_i\\) is the proportion of data belonging to class \\(i\\).\nEntropy: Originating from information theory, this is another metric for measuring data impurity. Like Gini Impurity, a value of 0 indicates perfect purity, while a higher value indicates more impurity. \\[\nH(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n\\]\nInformation Gain is the metric that shows “how much the impurity decreased” as a result of splitting the data by a certain question (feature). At each step, the decision tree selects the question that maximizes this Information Gain.\nInformation Gain is calculated with the following formula:\n\nInformation Gain = Impurity before split – (Weighted average of impurity of each node after split)\n\nIn other words, a split that creates purer groups (groups with lower impurity) is judged to be a “good split.”\n\n\n\n\n\n\n\nLet’s consider a small dataset used to predict “whether to play tennis or not” based on weather data.\n\nEntire Dataset (10 instances):\n\nPlay Tennis: 5\nDon’t Play Tennis: 5\n\n\n\n\n\n\nFirst, we calculate the Gini Impurity of the entire dataset (the root node).\n\n\\[Gini_{root} = 1 - ((\\frac{5}{10})^2 + (\\frac{5}{10})^2) = 1 - (0.25 + 0.25) = \\mathbf{0.5}\\]\n\n\n\n\nNow, let’s assume we split the data based on whether the “Wind is Weak or Strong.”\n\nWeak Wind Group (6 instances):\n\nPlay Tennis: 5\nDon’t Play Tennis: 1\n\\(Gini_{weak} = 1 - ((\\frac{5}{6})^2 + (\\frac{1}{6})^2) \\approx 1 - (0.694 + 0.028) = \\mathbf{0.278}\\)\n\nStrong Wind Group (4 instances):\n\nPlay Tennis: 0\nDon’t Play Tennis: 4\n\\(Gini_{strong} = 1 - ((\\frac{0}{4})^2 + (\\frac{4}{4})^2) = 1 - (0 + 1) = \\mathbf{0}\\)\n\n\n\n\n\n\n\nCalculate the weighted average Gini Impurity after the split.\n\n\\[Gini_{after\\_split} = (\\frac{6}{10}) \\times 0.278 + (\\frac{4}{10}) \\times 0 = 0.1668 + 0 = 0.1668\\]\n\nFinally, calculate the Information Gain.\n\n\\[\\text{Information Gain} = Gini_{root} - Gini_{after\\_split} = 0.5 - 0.1668 = \\mathbf{0.3332}\\]\n\nThis value of 0.3332 represents the “goodness” of the split using the “Wind” feature. The decision tree performs similar calculations for other features (like Outlook, Humidity, etc.) and chooses the split rule that results in the highest Information Gain.\n\n\n\n\n\n\n\n\nTheoretically, it is possible to create an infinite number of decision trees. There are two main reasons for this:\n\nPerfectly Memorizing Data (Overfitting): If no limits are placed on the tree’s depth or splitting, the algorithm will continue splitting until every single sample in the training data is isolated in its own leaf node. This results in a model that is 100% accurate on the training data but is useless for predicting new, unseen data. This creates an overly complex tree known as an overfit model.\nFeatures with Continuous Values: When features contain continuous values like “temperature” or “humidity,” there are infinitely many potential split points (e.g., split at Temp &gt; 25.0°C, Temp &gt; 25.01°C, etc.). This theoretically allows for the creation of infinitely many different tree structures.\n\nIn practice, to prevent such overfitting, techniques called “Pruning” are used to properly control the tree’s complexity. This includes limiting the maximum depth of the tree or setting a minimum number of samples required to be at a leaf node."
  },
  {
    "objectID": "06_decision_trees.html#decision-trees-dts",
    "href": "06_decision_trees.html#decision-trees-dts",
    "title": "Decision Trees",
    "section": "",
    "text": "A Decision Tree is a supervised learning model that uses a tree-like structure similar to a flowchart to classify or regress data. It arrives at a final prediction by repeatedly asking a series of “if-then-else” questions. A major advantage of this model is that its results are very easy to interpret due to its intuitive structure.\nA decision tree is composed of the following elements:\n\nRoot Node: The starting point of the tree, representing the first question.\nDecision Node: An intermediate node that represents an intermediate question used to further split the data.\nLeaf Node (or Terminal Node): The endpoint of a branch, representing the final predicted outcome (a class label or a numerical value).\n\nThis model is utilized in a wide range of fields, including customer purchase prediction, medical diagnosis, and credit scoring.\n\n\n\n\n\n\n\n\nA visual comparison of machine learning models. Supervised learning (left), which includes algorithms like Decision Trees, uses labeled data to classify a new point. In contrast, unsupervised learning (right) finds hidden clusters in unlabeled data. Source: Wikimedia Commons\n\n\n\n\n\n\n\n\nA decision tree model predicting Titanic passenger survival. The structure flows from the initial root node (‘gender’), through intermediate decision nodes (‘age,’ ‘sibsp’), to the final predictions in the leaf nodes (‘survived’/‘died’). Source: Wikimedia Commons"
  },
  {
    "objectID": "06_decision_trees.html#gini-impurity-entropy-and-information-gain",
    "href": "06_decision_trees.html#gini-impurity-entropy-and-information-gain",
    "title": "Decision Trees",
    "section": "",
    "text": "At each step, a decision tree must determine “which question will most efficiently split the data.” To make this judgment, it uses metrics like Gini Impurity or Entropy. These metrics measure the “impurity” or “disorder” of the data within a node.\nGini Impurity: This metric indicates how mixed the classes are within a node. A value close to 0 means the node is “pure” (contains data from mostly one class), while a higher value means it is “impure” (contains a mix of different classes). It is known for being relatively fast to compute. \\[\nGini(S) = 1 - \\sum_{i=1}^{c} (p_i)^2\n\\] where \\(p_i\\) is the proportion of data belonging to class \\(i\\).\nEntropy: Originating from information theory, this is another metric for measuring data impurity. Like Gini Impurity, a value of 0 indicates perfect purity, while a higher value indicates more impurity. \\[\nH(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n\\]\nInformation Gain is the metric that shows “how much the impurity decreased” as a result of splitting the data by a certain question (feature). At each step, the decision tree selects the question that maximizes this Information Gain.\nInformation Gain is calculated with the following formula:\n\nInformation Gain = Impurity before split – (Weighted average of impurity of each node after split)\n\nIn other words, a split that creates purer groups (groups with lower impurity) is judged to be a “good split.”"
  },
  {
    "objectID": "06_decision_trees.html#a-simple-example-of-measuring-a-splits-goodness",
    "href": "06_decision_trees.html#a-simple-example-of-measuring-a-splits-goodness",
    "title": "Decision Trees",
    "section": "",
    "text": "Let’s consider a small dataset used to predict “whether to play tennis or not” based on weather data.\n\nEntire Dataset (10 instances):\n\nPlay Tennis: 5\nDon’t Play Tennis: 5\n\n\n\n\n\n\nFirst, we calculate the Gini Impurity of the entire dataset (the root node).\n\n\\[Gini_{root} = 1 - ((\\frac{5}{10})^2 + (\\frac{5}{10})^2) = 1 - (0.25 + 0.25) = \\mathbf{0.5}\\]\n\n\n\n\nNow, let’s assume we split the data based on whether the “Wind is Weak or Strong.”\n\nWeak Wind Group (6 instances):\n\nPlay Tennis: 5\nDon’t Play Tennis: 1\n\\(Gini_{weak} = 1 - ((\\frac{5}{6})^2 + (\\frac{1}{6})^2) \\approx 1 - (0.694 + 0.028) = \\mathbf{0.278}\\)\n\nStrong Wind Group (4 instances):\n\nPlay Tennis: 0\nDon’t Play Tennis: 4\n\\(Gini_{strong} = 1 - ((\\frac{0}{4})^2 + (\\frac{4}{4})^2) = 1 - (0 + 1) = \\mathbf{0}\\)\n\n\n\n\n\n\n\nCalculate the weighted average Gini Impurity after the split.\n\n\\[Gini_{after\\_split} = (\\frac{6}{10}) \\times 0.278 + (\\frac{4}{10}) \\times 0 = 0.1668 + 0 = 0.1668\\]\n\nFinally, calculate the Information Gain.\n\n\\[\\text{Information Gain} = Gini_{root} - Gini_{after\\_split} = 0.5 - 0.1668 = \\mathbf{0.3332}\\]\n\nThis value of 0.3332 represents the “goodness” of the split using the “Wind” feature. The decision tree performs similar calculations for other features (like Outlook, Humidity, etc.) and chooses the split rule that results in the highest Information Gain."
  },
  {
    "objectID": "06_decision_trees.html#why-is-it-possible-to-create-an-infinite-number-of-trees",
    "href": "06_decision_trees.html#why-is-it-possible-to-create-an-infinite-number-of-trees",
    "title": "Decision Trees",
    "section": "",
    "text": "Theoretically, it is possible to create an infinite number of decision trees. There are two main reasons for this:\n\nPerfectly Memorizing Data (Overfitting): If no limits are placed on the tree’s depth or splitting, the algorithm will continue splitting until every single sample in the training data is isolated in its own leaf node. This results in a model that is 100% accurate on the training data but is useless for predicting new, unseen data. This creates an overly complex tree known as an overfit model.\nFeatures with Continuous Values: When features contain continuous values like “temperature” or “humidity,” there are infinitely many potential split points (e.g., split at Temp &gt; 25.0°C, Temp &gt; 25.01°C, etc.). This theoretically allows for the creation of infinitely many different tree structures.\n\nIn practice, to prevent such overfitting, techniques called “Pruning” are used to properly control the tree’s complexity. This includes limiting the maximum depth of the tree or setting a minimum number of samples required to be at a leaf node."
  },
  {
    "objectID": "06_decision_trees.html#training-and-test-data-for-yoy-inflation-prediction-high_inflation_yoy-derived-from-pcepilfe_yoy",
    "href": "06_decision_trees.html#training-and-test-data-for-yoy-inflation-prediction-high_inflation_yoy-derived-from-pcepilfe_yoy",
    "title": "Decision Trees",
    "section": "Training and Test Data for YoY Inflation Prediction (high_inflation_YoY derived from PCEPILFE_YoY)",
    "text": "Training and Test Data for YoY Inflation Prediction (high_inflation_YoY derived from PCEPILFE_YoY)\n\n\nFeature Engineering: To capture past trends and volatility, 9-month rolling window statistics (e.g., mean, standard deviation) were calculated for key economic indicators and added as features. For this specific model, no lag variables were used.\nDefining Predictors and Target: The binarized YoY inflation rate (high_inflation_YoY) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The Training Data for YoY Inflation\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The Test Data for YoY Inflation"
  },
  {
    "objectID": "06_decision_trees.html#training-and-test-data-for-mom-inflation-prediction-high_inflation_mom-derived-from-pcepilfe_mom",
    "href": "06_decision_trees.html#training-and-test-data-for-mom-inflation-prediction-high_inflation_mom-derived-from-pcepilfe_mom",
    "title": "Decision Trees",
    "section": "Training and Test Data for MoM Inflation Prediction (high_inflation_MoM derived from PCEPILFE_MoM)",
    "text": "Training and Test Data for MoM Inflation Prediction (high_inflation_MoM derived from PCEPILFE_MoM)\n\n\nFeature Engineering: To capture past trends and volatility, 4-month rolling window statistics (e.g., mean, standard deviation) were calculated for key economic indicators and added as features. For this specific model, no lag variables were used.\nDefining Predictors and Target: The binarized MoM inflation rate (high_inflation_MoM) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The Training Data for MoM Inflation\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The Test Data for MoM Inflation"
  },
  {
    "objectID": "06_decision_trees.html#training-and-test-data-for-unemployment-rates-high_unemployment-derived-from-unrate",
    "href": "06_decision_trees.html#training-and-test-data-for-unemployment-rates-high_unemployment-derived-from-unrate",
    "title": "Decision Trees",
    "section": "Training and Test Data for Unemployment Rates (high_unemployment derived from UNRATE)",
    "text": "Training and Test Data for Unemployment Rates (high_unemployment derived from UNRATE)\n\n\nFeature Engineering: To provide the model with historical context, the feature set was expanded. Specifically, 8-month lag variables and 3-month rolling window statistics (e.g., mean, standard deviation) were calculated for each economic indicator and added as features.\nDefining Predictors and Target: The binarized unemployment rate (high_unemployment) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The Training Data for Unemployment Rates\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The Test Data for Unemployment Rates"
  },
  {
    "objectID": "06_decision_trees.html#confusion-matrix-for-yoy-inflation-prediction",
    "href": "06_decision_trees.html#confusion-matrix-for-yoy-inflation-prediction",
    "title": "Decision Trees",
    "section": "Confusion Matrix for YoY Inflation Prediction",
    "text": "Confusion Matrix for YoY Inflation Prediction\n\n\nAccuracy: 92.5%\nThis is an exceptionally high-performing and well-balanced model. Its primary strength is that both precision and recall are above 90% for both the “High Inflation” and “Low Inflation” classes.\nThis indicates the model is simultaneously excellent at finding nearly all high inflation periods (94% recall) and ensuring that its predictions are highly reliable (91% precision). With only 5 total misclassifications, this Decision Tree model serves as an ideal and effective classifier for this task.\n\n\n\n\n\n\n\n\n\nFigure 7: Confusion Matrix for YoY Inflation Prediction"
  },
  {
    "objectID": "06_decision_trees.html#confusion-matrix-for-mom-inflation-prediction",
    "href": "06_decision_trees.html#confusion-matrix-for-mom-inflation-prediction",
    "title": "Decision Trees",
    "section": "Confusion Matrix for MoM Inflation Prediction",
    "text": "Confusion Matrix for MoM Inflation Prediction\n\n\nAccuracy: 65.2%\nThis model’s performance is poor, with an accuracy only slightly better than a random guess.\nIts primary weakness is low precision across both classes (e.g., 63% for “High Inflation”), which means its predictions are unreliable. When the model signals “High Inflation,” it is incorrect more than a third of the time.\nAlthough it manages to find a majority of the high inflation periods (71% recall), it does so by generating a high number of false alarms. Overall, the model lacks the reliability required for practical use.\n\n\n\n\n\n\n\n\n\nFigure 8: Confusion Matrix for MoM Inflation Prediction"
  },
  {
    "objectID": "06_decision_trees.html#confusion-matrix-for-unemployment-rates-prediction",
    "href": "06_decision_trees.html#confusion-matrix-for-unemployment-rates-prediction",
    "title": "Decision Trees",
    "section": "Confusion Matrix for Unemployment Rates Prediction",
    "text": "Confusion Matrix for Unemployment Rates Prediction\n\n\nAccuracy: 97.0%\n\n-This is an exceptionally high-performing and reliable model, demonstrating near-perfect results.\n-Its most notable strengths are two perfect metrics: 1. 100% precision for “High Unemployment,” which means its predictions of a high unemployment period are never wrong. 2. 100% recall for “Low Unemployment,” meaning it never misclassifies a low unemployment period.\n\nThe model’s only minor imperfection is missing 2 out of 32 high unemployment events, still resulting in a very strong 94% recall for that class. Overall, this Decision Tree model shows nearly ideal performance for this task.\n\n\n\n\n\n\n\n\n\nFigure 9: Confusion Matrix for Unemployment Rates Prediction"
  },
  {
    "objectID": "06_decision_trees.html#related-to-q5-out-of-10-research-questions-in-introduction",
    "href": "06_decision_trees.html#related-to-q5-out-of-10-research-questions-in-introduction",
    "title": "Decision Trees",
    "section": "Related to Q5 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q5 (out of 10 Research Questions) in Introduction\n\n\nRefer to the conclusion below."
  },
  {
    "objectID": "04_pca.html",
    "href": "04_pca.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Notes: All images displayed on this page are free-use materials, unless otherwise indicated by a specific credit."
  },
  {
    "objectID": "04_pca.html#related-to-q3-out-of-10-research-questions-in-introduction",
    "href": "04_pca.html#related-to-q3-out-of-10-research-questions-in-introduction",
    "title": "Principal Component Analysis (PCA)",
    "section": "Related to Q3 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q3 (out of 10 Research Questions) in Introduction\n\nHow do macroeconomic factors extracted through principal component analysis (PCA) affect the dynamics of inflation and unemployment?\n\nAccording to Figure 2, macroeconomic factors extracted through PCA affect the dynamics of inflation and unemployment mainly through two channels:\n\nInflation factor (first principal component): When this factor rises, both the year-over-year (YoY) and month-over-month (MoM) core inflation rates increase, while the unemployment rate decreases. This reflects a short-run Phillips curve relationship, namely higher inflationary pressures → lower unemployment.\nBusiness cycle factors (second and third principal components): These are closely linked to real economic activity (production, retail sales, and labor market indicators) and represent business cycle conditions by primarily driving unemployment fluctuations. Their effects on inflation generally emerge with a lag through the demand channel, with MoM inflation responding earlier than YoY inflation, as it is more sensitive to short-term fluctuations.\n\nIn summary, PCA facilitates the identification of distinct macroeconomic drivers, isolating an inflation factor and business cycle factors, and thereby providing a structured account of how persistent price pressures and cyclical labor market conditions jointly shape the evolution of inflation and unemployment."
  },
  {
    "objectID": "02_03_data_visualization.html",
    "href": "02_03_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Figure 1: Descriptive Statistics of Macroeconomic Indicators. The summary highlights central tendencies and variability across variables, revealing substantial dispersion in indicators such as industrial production (INDPRO) and retail sales (RSAFS), while others like labor force participation (CIVPART) and inflation (PCEPILFE_YoY and PCEPILFE_MoM) show more stable ranges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of Macroeconomic Indicators. The distributions reveal that many variables, such as inflation (PCEPILFE_YoY and PCEPILFE_MoM) and labor force participation (CIVPART), cluster around specific ranges with relatively symmetric shapes, while others like unemployment rate (UNRATE) and money supply (M2SL) display skewness or heavy tails, indicating the presence of outliers and structural differences across indicators.\n\n\n\n\n\n\n\n\n\n\n\nA value of 1 is a recessionary period, while a value of 0 is an expansionary period.\n\n\n\n\n\n\n\n\n\nFigure 3: Violin Plots of Macroeconomic Indicators Grouped by USREC. The distributions show that during recession periods (USREC=1), unemployment (UNRATE) shifts upward in its violin plot, while inflation (PCEPILFE_YoY and PCEPILFE_MoM) and output growth (GDPC1) violins are generally lower, reflecting typical cyclical dynamics of economic downturns.\n\n\n\n\n\n\n\n\n\nA value of 1 is a zero lower bound constraint period, while a value of 0 is not.\n\n\n\n\n\n\n\n\n\nFigure 4: Violin Plots of Macroeconomic Indicators Grouped by ZLB_dummy. The plots show that under the zero lower bound period (ZLB_dummy=1), the federal funds rate (FEDFUNDS) violin collapses near zero by definition, while the unemployment rate (UNRATE) violin shifts upward, indicating looser monetary conditions accompanied by labor market weakness.\n\n\n\n\n\n\n\n\n\nA value of 1 is a COVID period, while a value of 0 is not.\n\n\n\n\n\n\n\n\n\nFigure 5: Violin Plots of Macroeconomic Indicators Grouped by COVID_dummy. The plots show that during the COVID period (COVID_dummy=1), unemployment (UNRATE) rises sharply while labor force participation (CIVPART) declines, and output growth (GDPC1) and industrial production (INDPRO) distributions widen considerably, reflecting heightened volatility and economic disruption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correlation Heatmap of Macroeconomic Indicators. The plot shows both positive (red) and negative (blue) correlations, highlighting strong positive relationships such as between inflation (PCEPILFE_YoY and PCEPILFE_MoM) and inflation expectations (T5YIE, T10YIE, PX_MD, and PX5_MD), and strong negative relationships such as between unemployment rate (UNRATE) and the federal funds effective rate (FEDFUNDS), which can guide variable selection and interpretation in further analysis."
  },
  {
    "objectID": "02_03_data_visualization.html#summary-statistics",
    "href": "02_03_data_visualization.html#summary-statistics",
    "title": "Data Visualization",
    "section": "",
    "text": "Figure 1: Descriptive Statistics of Macroeconomic Indicators. The summary highlights central tendencies and variability across variables, revealing substantial dispersion in indicators such as industrial production (INDPRO) and retail sales (RSAFS), while others like labor force participation (CIVPART) and inflation (PCEPILFE_YoY and PCEPILFE_MoM) show more stable ranges."
  },
  {
    "objectID": "02_03_data_visualization.html#histograms",
    "href": "02_03_data_visualization.html#histograms",
    "title": "Data Visualization",
    "section": "",
    "text": "Figure 2: Histograms of Macroeconomic Indicators. The distributions reveal that many variables, such as inflation (PCEPILFE_YoY and PCEPILFE_MoM) and labor force participation (CIVPART), cluster around specific ranges with relatively symmetric shapes, while others like unemployment rate (UNRATE) and money supply (M2SL) display skewness or heavy tails, indicating the presence of outliers and structural differences across indicators."
  },
  {
    "objectID": "02_03_data_visualization.html#violin-plots-of-macroeconomic-indicators",
    "href": "02_03_data_visualization.html#violin-plots-of-macroeconomic-indicators",
    "title": "Data Visualization",
    "section": "",
    "text": "A value of 1 is a recessionary period, while a value of 0 is an expansionary period.\n\n\n\n\n\n\n\n\n\nFigure 3: Violin Plots of Macroeconomic Indicators Grouped by USREC. The distributions show that during recession periods (USREC=1), unemployment (UNRATE) shifts upward in its violin plot, while inflation (PCEPILFE_YoY and PCEPILFE_MoM) and output growth (GDPC1) violins are generally lower, reflecting typical cyclical dynamics of economic downturns.\n\n\n\n\n\n\n\n\n\nA value of 1 is a zero lower bound constraint period, while a value of 0 is not.\n\n\n\n\n\n\n\n\n\nFigure 4: Violin Plots of Macroeconomic Indicators Grouped by ZLB_dummy. The plots show that under the zero lower bound period (ZLB_dummy=1), the federal funds rate (FEDFUNDS) violin collapses near zero by definition, while the unemployment rate (UNRATE) violin shifts upward, indicating looser monetary conditions accompanied by labor market weakness.\n\n\n\n\n\n\n\n\n\nA value of 1 is a COVID period, while a value of 0 is not.\n\n\n\n\n\n\n\n\n\nFigure 5: Violin Plots of Macroeconomic Indicators Grouped by COVID_dummy. The plots show that during the COVID period (COVID_dummy=1), unemployment (UNRATE) rises sharply while labor force participation (CIVPART) declines, and output growth (GDPC1) and industrial production (INDPRO) distributions widen considerably, reflecting heightened volatility and economic disruption."
  },
  {
    "objectID": "02_03_data_visualization.html#correlation-heatmap",
    "href": "02_03_data_visualization.html#correlation-heatmap",
    "title": "Data Visualization",
    "section": "",
    "text": "Figure 6: Correlation Heatmap of Macroeconomic Indicators. The plot shows both positive (red) and negative (blue) correlations, highlighting strong positive relationships such as between inflation (PCEPILFE_YoY and PCEPILFE_MoM) and inflation expectations (T5YIE, T10YIE, PX_MD, and PX5_MD), and strong negative relationships such as between unemployment rate (UNRATE) and the federal funds effective rate (FEDFUNDS), which can guide variable selection and interpretation in further analysis."
  },
  {
    "objectID": "02_03_data_visualization.html#time-series-of-macroeconomic-indicators",
    "href": "02_03_data_visualization.html#time-series-of-macroeconomic-indicators",
    "title": "Data Visualization",
    "section": "Time Series of Macroeconomic Indicators",
    "text": "Time Series of Macroeconomic Indicators\n\n\n\n\n\n\n\nFigure 7: Time Series of Macroeconomic Indicators. The plots reveal diverse dynamics across variables, with some showing steady long-term trends while others exhibit sharp fluctuations or structural breaks, reflecting economic cycles and major events such as recessions or policy shifts."
  },
  {
    "objectID": "02_03_data_visualization.html#locally-estimated-scatterplot-smoothing-loess",
    "href": "02_03_data_visualization.html#locally-estimated-scatterplot-smoothing-loess",
    "title": "Data Visualization",
    "section": "Locally Estimated Scatterplot Smoothing (LOESS)",
    "text": "Locally Estimated Scatterplot Smoothing (LOESS)\n\n\n\n\n\n\n\nFigure 8: LOESS-Smoothed Trends of Macroeconomic Indicators. The orange smoothed lines highlight underlying trajectories across variables, effectively filtering out short-term noise and revealing the baseline movement of each indicator."
  },
  {
    "objectID": "02_03_data_visualization.html#hodrickprescott-filter-hp-filtrer-trend",
    "href": "02_03_data_visualization.html#hodrickprescott-filter-hp-filtrer-trend",
    "title": "Data Visualization",
    "section": "Hodrick–Prescott Filter (HP Filtrer): Trend",
    "text": "Hodrick–Prescott Filter (HP Filtrer): Trend\n\n\n\n\n\n\n\nFigure 9: HP-Trend Decomposition of Macroeconomic Indicators. The orange lines extracted by the HP filter reveal the long-term trend components of each series, smoothing out short-term fluctuations and highlighting gradual shifts such as the steady decline in the natural rate of unemployment (NROU)."
  },
  {
    "objectID": "02_03_data_visualization.html#hodrickprescott-filter-hp-filtrer-cycle",
    "href": "02_03_data_visualization.html#hodrickprescott-filter-hp-filtrer-cycle",
    "title": "Data Visualization",
    "section": "Hodrick–Prescott Filter (HP Filtrer): Cycle",
    "text": "Hodrick–Prescott Filter (HP Filtrer): Cycle\n\n\n\n\n\n\n\nFigure 10: HP-Cycle Components of Macroeconomic Indicators. The plots show short-term deviations from long-term trends, with most series fluctuating around zero, indicating cyclical dynamics and temporary shocks rather than persistent structural shifts."
  },
  {
    "objectID": "02_03_data_visualization.html#autocorrelation-function-acf",
    "href": "02_03_data_visualization.html#autocorrelation-function-acf",
    "title": "Data Visualization",
    "section": "Autocorrelation Function (ACF)",
    "text": "Autocorrelation Function (ACF)\n\n\n\n\n\n\n\nFigure 11: Autocorrelation Functions (ACF) of Macroeconomic Indicators with 36 Monthly Lags. The plots reveal persistent autocorrelation in many variables such as unemployment (UNRATE and NROU) and inflation measures (PCEPILFE_YoY and PCEPILFE_MoM), suggesting strong serial dependence, while others like industrial production (INDPRO) and retail sales (RSAFS) exhibit weaker or rapidly decaying correlations, indicating more short-term variability."
  },
  {
    "objectID": "02_03_data_visualization.html#partial-autocorrelation-function-pacf",
    "href": "02_03_data_visualization.html#partial-autocorrelation-function-pacf",
    "title": "Data Visualization",
    "section": "Partial Autocorrelation Function (PACF)",
    "text": "Partial Autocorrelation Function (PACF)\n\n\n\n\n\n\n\nFigure 12: Partial Autocorrelation Functions (PACF) of Macroeconomic Indicators with 36 Monthly Lags. Many variables show significant spikes only at the first few lags—typical of autoregressive processes—while others display more persistent or irregular patterns, indicating potential higher-order dynamics or structural shocks in the data."
  },
  {
    "objectID": "02_01_overview.html",
    "href": "02_01_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Notes: For the code used in this chapter (DataPrep/EDA), please refer here. This chapter uses the data available as of September 8, 2025.\n\n\nData\n\n\nIn order to address the 10 Research Questions presented in Introduction and thereby achieve the objectives of this study, the data series listed in the Table 1 were collected. The table shows the category, frequency, data source, and retrieval method for each variable. In the Frequency column, “D,” “W,” “M,” and “Q” indicate daily, weekly, monthly, and quarterly, respectively. By clicking the hyperlink attached to each data source in the Source column, you can access the corresponding data source.\n\nAn example of a URL GET request with SERIES_ID, API_KEY, and YYYY-MM-DD when using the FRED API is as follows:\n\nFor example, if you want to retrieve the Core PCE series from January 1, 1990 onward, substitute 'PCEPILFE' for SERIES_ID and '1990-01-01' for YYYY-MM-DD. For API_KEY, enter your own API key.\n\nhttps://api.stlouisfed.org/fred/series/observations?series_id=SERIES_ID&api_key=API_KEY&file_type=json&observation_start=YYYY-MM-DD\nThe 1-year and 5-year Expected Changes in Inflation Rates were downloaded as CSV files from the University of Michigan Surveys of Consumers website. By clicking the hyperlink attached to each variable name, the raw data can be downloaded.\nNote that the COVID-19 period dummy was constructed according to the time period, which takes the value of 1 from April 2020 to March 2021 and 0 otherwise.\n\n\n\n\n\n\nTable 1: Data List\n\n\n\n\n\nCategroy\nVariable\nFrequency\nSource\nMethod of Acquisition\n\n\n\n\nInflation\nCore PCE\nM\nFRED\nAPI fetch\n\n\nUnemployment\nUnemployment Rate\nM\nFRED\nAPI fetch\n\n\nUnemployment\nNoncyclical Rate of Unemployment\nQ\nFRED\nAPI fetch\n\n\nBusiness Cycles and Policy Regimes\nRecession Indicators\nM\nFRED\nAPI fetch\n\n\nBusiness Cycles and Policy Regimes\nZero Lower Bound dummy\nM\nFRED\nAPI fetch\n\n\nBusiness Cycles and Policy Regimes\nCOVID-19 Period dummy\nM\nNot Available\nCreated\n\n\nDemand\nReal GDP\nQ\nFRED\nAPI fetch\n\n\nDemand\nReal Potential GDP\nQ\nFRED\nAPI fetch\n\n\nDemand\nIndustrial Production\nM\nFRED\nAPI fetch\n\n\nDemand\nRetail Sales\nM\nFRED\nAPI fetch\n\n\nSupply\nCrude Oil Prices\nM\nFRED\nAPI fetch\n\n\nSupply\nImport Price Index\nM\nFRED\nAPI fetch\n\n\nSupply\nLabor Productivity\nQ\nFRED\nAPI fetch\n\n\nLabor Markets\nAverage Hourly Earnings\nM\nFRED\nAPI fetch\n\n\nLabor Markets\nLabor Force Participation Rate\nM\nFRED\nAPI fetch\n\n\nLabor Markets\nJob Openings\nM\nFRED\nAPI fetch\n\n\nMonetary Policy\nFederal Funds Effective Rate\nM\nFRED\nAPI fetch\n\n\nMonetary Policy\nMoney Suppley (M2)\nM\nFRED\nAPI fetch\n\n\nMonetary Policy\nTotal Assets of Federal Reserve\nW\nFRED\nAPI fetch\n\n\nInflation Expectations\n5-Year Breakeven Inflation Rate\nD\nFRED\nAPI fetch\n\n\nInflation Expectations\n10-Year Breakeven Inflation Rate\nD\nFRED\nAPI fetch\n\n\nInflation Expectations\n1-Year Expected Changes in Inflation Rates\nM\nUniversity of Michigan\nCSV download\n\n\nInflation Expectations\n5-Year Expected Changes in Inflation Rates\nM\nUniversity of Michigan\nCSV download"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Dynamics Between Inflation and Unemployment: Empirical Evidence from U.S. Macroeconomic Data",
    "section": "",
    "text": "Welcome\n\n\nThis website presents the process and results of analyses related to the research theme of The Dynamics Between Inflation and Unemployment: Empirical Evidence from U.S. Macroeconomic Data. You can browse the sections of interest from the tabs above.\n\n\n\n\nAbout Me\n\n\n\n\n\n\nThe author of this website, Taihei Sone, received the B.S. degree in Engineering from the University of Tokyo, Tokyo, Japan, in 2017, and the M.S. degree in Engineering from the University of Tokyo in 2019, where he was awarded the Department Chair’s Award for Outstanding Academic Achievement. Since 2024, he has been pursuing the M.S. degree in Data Science at the University of Colorado Boulder, Boulder, CO, USA. His research interests include machine learning, large-scale data systems, and artificial intelligence applications in economics.\nYou can also find him on  and ."
  },
  {
    "objectID": "01_introduction.html",
    "href": "01_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Notes: All images displayed on this page are free-use materials, unless otherwise indicated by a specific credit.\n\n\nBackgrounds\n\n\nThe relationship between inflation and unemployment has long constituted a central issue in the field of macroeconomics. These two indicators are not merely statistical constructs but rather fundamental determinants that directly affect the standard of living of households and the overall stability of society. Inflation reflects the purchasing power of money and exerts a profound influence on the decision-making processes of households and firms, while unemployment represents the state of labor market conditions and is closely associated with household income and the productive capacity of the economy. Historically, the evolution of these two variables has frequently posed conflicting challenges for policymakers. For example, expansionary policies may succeed in reducing unemployment in the short run, yet they simultaneously raise the likelihood of accelerating inflation. Conversely, policies aimed at containing inflation often depress aggregate demand, thereby heightening the risk of increased unemployment. Thus, reconciling the goals of price stability and full employment has consistently been a central concern in both economic theory and policy practice. The U.S. Federal Reserve’s explicit commitment to a “dual mandate” of price stability and maximum employment exemplifies this tension. Moreover, this dual mandate extends beyond considerations of economic efficiency alone, carrying critical implications for social cohesion and political stability. An uncontrolled surge in inflation can undermine household welfare and destabilize everyday life, while persistently elevated unemployment can exacerbate inequality, fuel political unrest, and highlight the need for a comprehensive theoretical and empirical understanding of their dynamics for effective policy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a classical contribution to this theme, Phillips (1958) demonstrated the existence of a stable negative correlation between wage growth and the unemployment rate, thereby presenting the important insight that the balance of supply and demand in the labor market is directly linked to wage fluctuations. His study was based on long-term data from the United Kingdom, but the result came to be regarded as a general phenomenon observable in many countries. Subsequently, Samuelson and Solow (1960), through an empirical study using U.S. data, generalized Phillips’s finding into a trade-off relationship between inflation and unemployment, emphasizing the possibility of applying this relationship to economic policy and suggesting that policymakers could “choose” a desirable combination of the two. This interpretation had a major impact on macroeconomic policy debates at the time, but the stability of the relationship was questioned early on, and a series of theoretical objections soon emerged. Muth (1961), with the rational expectations hypothesis, challenged the traditional adaptive expectations framework by assuming that agents form expectations consistent with the underlying economic model. Furthermore, Sargent and Wallace (1975) advanced the policy ineffectiveness proposition, arguing that anticipated monetary policy has no lasting impact on real economic activity, and Lucas (1976) added the critique that statistically observed macroeconomic relationships may shift depending on the prevailing policy rule. These arguments highlighted the limitations of using the Phillips curve directly for policy purposes and led to the understanding that there is no sustained long-run trade-off between inflation and unemployment, with the unemployment rate converging to its natural rate. At the same time, since the neoclassical framework assuming perfectly flexible prices could not sufficiently explain short-run business cycle fluctuations, the need for more realistic models increased. Against this background, New Keynesian models such as the price adjustment cost model by Rotemberg (1982) and the stochastic price-setting model by Calvo (1983) emerged, demonstrating that by explicitly incorporating nominal rigidities, a short-run trade-off between inflation and unemployment could indeed exist. These models were subsequently systematized by Woodford (2003), forming the standard theoretical foundation of modern monetary policy analysis and becoming widely referenced in the practice of central banking.\n\n\n\n\n\n\nSource: WallStreetMojo\n\n\n\n\n\nOn the empirical side, the strength and stability of the relationship between inflation and unemployment have consistently been a subject of debate. In particular, recent studies indicate that the Phillips curve does not function as a fixed empirical regularity but rather changes its shape depending on the historical period and economic environment. Ball and Mazumder (2011), using U.S. data, reported a flattening of the Phillips curve and demonstrated that the impact of unemployment fluctuations on inflation has weakened substantially compared to the past. This finding is consistent with the anchoring of inflation expectations and the increased credibility of monetary policy, and it has been recognized as a factor contributing to the stability of inflation. Blanchard (2016) focused on U.S. inflation dynamics and emphasized that, unlike in earlier periods, changes in unemployment no longer move inflation rates as strongly, making it more difficult for policymakers to rely on the Phillips curve. Furthermore, Stock and Watson (2019) showed that the predictability of U.S. inflation has declined markedly in recent years and concluded that traditional Phillips curve–based forecasting models fail to deliver sufficient accuracy. Hazell et al. (2022), using state-level panel data, provided empirical evidence that the relationship between inflation and unemployment changes nonlinearly depending on whether the economy is subject to demand or supply shocks. Their research is important in that it highlights how relationships that appear blurred in aggregate macroeconomic data can become more clearly identifiable at the regional level. Taken together, this body of work suggests that the Phillips curve does not simply operate as a stable relationship in which “lower unemployment leads to higher inflation,” but instead varies significantly depending on economic conditions, the nature of shocks, and policy regimes. Therefore, analyzing the relationship between inflation and unemployment requires a flexible approach that goes beyond reliance on static empirical regularities and takes into account institutional contexts and the characteristics of shocks.\n\n\n\n\n\n\n\nIn recent years, with the advancement of machine learning, there has been a rapid increase in attempts to apply data-driven methods to the forecasting of key macroeconomic variables such as inflation and unemployment. These methods have attracted attention because they offer the potential to capture nonlinearities and complex interdependencies that traditional linear regression and time-series models fail to detect. Medeiros et al. (2021), using U.S. inflation data, employed Random Forests and Least Absolute Shrinkage and Selection Operator (LASSO) in their analysis and reported predictive accuracy superior to that of conventional AutoRegressive (AR) and factor models, thereby demonstrating that machine learning can hold an advantage over traditional approaches. This study provided results directly relevant to improving inflation forecasts used in practice by central banks and other policymakers, underscoring its empirical significance. Furthermore, Gogas et al. (2021) examined unemployment in the euro area as a whole and compared machine learning techniques with traditional time-series models, showing that machine learning has an advantage in short-term forecasting. They found that while Random Forests and Support Vector Machines (SVMs) achieve higher prediction accuracy, challenges remain regarding model interpretability and policy applicability, highlighting the difficulty of applying machine learning directly to policy operations. This issue points to a fundamental challenge in macroeconomic research—namely, how to balance predictive accuracy with theoretical consistency and policy relevance. In addition, Magazzino et al. (2025) analyzed unemployment across 23 advanced economies using artificial neural networks and demonstrated that factors such as GDP, labor productivity, demographic dynamics, and even AI-driven technological innovation are major determinants of unemployment fluctuations. Their results suggest that unemployment dynamics depend not only on cyclical factors but also heavily on structural and technological elements. Taken together, these studies underscore that while machine learning has proven effective for forecasting macroeconomic variables, they also highlight the need to overcome challenges related to model transparency and practical policy applicability.\n\n\n\n\n\n\n\nHowever, important challenges remain in this line of research. First, much of the existing literature has focused on either inflation or unemployment in isolation, and there are relatively few attempts to analyze both variables in an integrated manner and systematically capture their interactions. In fact, although inflation and unemployment should be considered simultaneously from both theoretical and policy perspectives, many empirical studies have tended to concentrate on only one side. Second, while many studies emphasize the evaluation of predictive accuracy, frameworks that combine regime classification with regression-based forecasting have not been sufficiently developed. For example, only a limited number of studies identify regimes such as high- and low-inflation periods or recessions and expansions, and then evaluate predictive performance within each of these states. Third, much of the data used are drawn from commercial databases or restricted sources, which imposes constraints on transparency and reproducibility, although the use of open data could create an environment in which a broader range of researchers and policymakers can more easily replicate and extend results. Few studies, however, have explicitly incorporated this consideration. Fourth, the linear regression models that continue to play a central role in policy analysis, such as Ordinary Least Squares (OLS) and Vector Auto Regressive (VAR), are rarely compared directly with machine learning methods within a unified framework. This lack of comparison has limited the perspective needed for policymakers to understand how traditional and modern methods might be used in a complementary fashion, and the absence of such evaluations represents a major obstacle to assessing the practical applicability of machine learning in real-world policy contexts. Therefore, future research must adopt a more comprehensive approach that addresses not only predictive accuracy but also interpretability and policy relevance at the same time.\n\n\n\n\n\n\n\nBased on the foregoing discussion, the objectives of this study can be organized into four main points. First, it aims to conduct an integrated analysis of inflation and unemployment by comparing traditional regression models (OLS, VAR, Elastic Net) with machine learning techniques (Naïve Bayes, decision trees, SVM, and neural networks) within a unified framework, thereby evaluating both predictive accuracy and interpretability. Second, it seeks to identify macroeconomic regimes through clustering and principal component analysis, and to apply classification and regression approaches in an integrated manner so as to examine simultaneously “where the economy currently stands” and “how it is likely to evolve.” Third, it will systematically analyze the dynamic causal relationship and regime dependence between inflation and unemployment using Granger causality tests and local projection methods. Fourth, by utilizing open and reproducible data sources such as the FRED API, the BLS API, and the World Bank API, the study aims to ensure transparency and reproducibility of the analysis and to provide a research platform that can be replicated and extended by a broad range of readers, including academic researchers and policymakers. Through these four pillars, this study aspires to bridge traditional research on the Phillips curve with contemporary machine learning approaches, thereby offering a new understanding of the relationship between inflation and unemployment. In addition, the study emphasizes not only the re-examination of existing findings but also the direct comparison between machine learning and regression analysis, with the explicit goal of deriving practical implications for policy applications. In light of this, the study sets forth 10 research questions designed to reconsider the relationship between inflation and unemployment from multiple perspectives, beginning with the existence of a short-run trade-off and expanding to assess regime dependence and the role of macroeconomic factors. Furthermore, by comparing the predictive performance and interpretability of machine learning and traditional models, they also delve into the causal direction of the relationship and the dynamic effects of shocks. Accordingly, this study seeks to provide a systematic framework that comprehensively addresses both theoretical and empirical challenges.\n\n\n\n\n\n\n\nResearch Questions\n\n\nIs a short-run trade-off between inflation and unemployment statistically observable?\nTo what extent does the Phillips curve relationship vary across business-cycle phases (recessions and expansions) and policy regimes (e.g., the zero lower bound period or the COVID-19 era)?\nHow do macroeconomic factors extracted through principal component analysis (PCA) affect the dynamics of inflation and unemployment?\nDoes regime identification based on data-driven clustering align with historical episodes such as stagflation or the post–global financial crisis recovery?\nTo what extent can simple classification models such as Naïve Bayes and decision trees accurately identify periods of high inflation or high unemployment?\nDoes the support vector machine (SVM) improve regime-classification accuracy compared with simpler classifiers such as Naïve Bayes and decision trees?\nHow do traditional regression models (OLS, VAR, Elastic Net) and machine learning models (SVM, neural networks) differ in their performance when forecasting inflation and unemployment one, three, and six months ahead?\nCan performance differences across forecasting models be judged statistically significant using tests such as the Diebold–Mariano test?\nAccording to Granger causality tests, what is the causal direction between inflation and unemployment (inflation → unemployment or unemployment → inflation)?\nThrough local projection analysis, how do demand shocks and supply shocks dynamically affect inflation and unemployment?"
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html",
    "href": "02_02_data_gathering_and_cleaning.html",
    "title": "Data Gathering and Data Cleaning",
    "section": "",
    "text": "The raw data are monthly data starting from January 1959, indexed so that the average for 2017 equals 100. The cleaned data consist of PCEPILFE_YoY, which represents the year-over-year percentage change of the raw data, and PCEPILFE_MoM, which represents the month-over-month annualized percentage change of the raw data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#core-pce",
    "href": "02_02_data_gathering_and_cleaning.html#core-pce",
    "title": "Data Gathering and Data Cleaning",
    "section": "",
    "text": "The raw data are monthly data starting from January 1959, indexed so that the average for 2017 equals 100. The cleaned data consist of PCEPILFE_YoY, which represents the year-over-year percentage change of the raw data, and PCEPILFE_MoM, which represents the month-over-month annualized percentage change of the raw data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#unemployment-rate",
    "href": "02_02_data_gathering_and_cleaning.html#unemployment-rate",
    "title": "Data Gathering and Data Cleaning",
    "section": "Unemployment Rate",
    "text": "Unemployment Rate\n\n\nThe raw data are monthly data starting from January 1948, expressed in percent. The cleaned data use these values directly; therefore, the two represent the same data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#noncyclical-rate-of-unemployment",
    "href": "02_02_data_gathering_and_cleaning.html#noncyclical-rate-of-unemployment",
    "title": "Data Gathering and Data Cleaning",
    "section": "Noncyclical Rate of Unemployment",
    "text": "Noncyclical Rate of Unemployment\n\n\nThe raw data are quarterly series starting from the first quarter of 1949, expressed in percent. The cleaned data convert them into monthly series using spline interpolation."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#recession-indicators",
    "href": "02_02_data_gathering_and_cleaning.html#recession-indicators",
    "title": "Data Gathering and Data Cleaning",
    "section": "Recession Indicators",
    "text": "Recession Indicators\n\n\nThe raw data are a monthly binary series starting from December 1854. A value of 1 is a recessionary period, while a value of 0 is an expansionary period. The cleaned data use these values directly; therefore, the two represent the same data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#zero-lower-bound-dummy",
    "href": "02_02_data_gathering_and_cleaning.html#zero-lower-bound-dummy",
    "title": "Data Gathering and Data Cleaning",
    "section": "Zero Lower Bound dummy",
    "text": "Zero Lower Bound dummy\n\n\nThe raw data are the Federal Funds Effective Rate, provided as monthly series starting from July 1954, expressed in percent. The cleaned data are monthly series that take the value of 1 when the raw data are 0.25 percent or less, and 0 otherwise."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#covid-19-period-dummy",
    "href": "02_02_data_gathering_and_cleaning.html#covid-19-period-dummy",
    "title": "Data Gathering and Data Cleaning",
    "section": "COVID-19 Period dummy",
    "text": "COVID-19 Period dummy\n\n\nThis data is a monthly binary series that takes the value of 1 from April 2020 to March 2021, and 0 otherwise. The data period is conservatively set from January 1800 to December 2099. No external raw data were obtained or used."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#real-gdp",
    "href": "02_02_data_gathering_and_cleaning.html#real-gdp",
    "title": "Data Gathering and Data Cleaning",
    "section": "Real GDP",
    "text": "Real GDP\n\n\nThe raw data are quarterly data starting from the first quarter of 1947, expressed in billions of chained 2017 dollars. The cleaned data are converted into monthly series by first expressing the quarter-to-quarter changes at an annualized rate in percent and then applying spline interpolation."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#real-potential-gdp",
    "href": "02_02_data_gathering_and_cleaning.html#real-potential-gdp",
    "title": "Data Gathering and Data Cleaning",
    "section": "Real Potential GDP",
    "text": "Real Potential GDP\n\n\nThe raw data are quarterly data starting from the first quarter of 1949, expressed in billions of chained 2017 dollars. The cleaned data are converted into monthly series by first expressing the quarter-to-quarter changes at an annualized rate in percent and then applying spline interpolation."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#industrial-production",
    "href": "02_02_data_gathering_and_cleaning.html#industrial-production",
    "title": "Data Gathering and Data Cleaning",
    "section": "Industrial Production",
    "text": "Industrial Production\n\n\nThe raw data are monthly data starting from January 1919, expressed as an index with the 2017 average set to 100. The cleaned data are monthly series showing the month-over-month changes at an annualized rate in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#retail-sales",
    "href": "02_02_data_gathering_and_cleaning.html#retail-sales",
    "title": "Data Gathering and Data Cleaning",
    "section": "Retail Sales",
    "text": "Retail Sales\n\n\nThe raw data are monthly data starting from January 1992, expressed in millions of dollars. The cleaned data are monthly series showing the month-over-month changes at an annualized rate in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#crude-oil-prices",
    "href": "02_02_data_gathering_and_cleaning.html#crude-oil-prices",
    "title": "Data Gathering and Data Cleaning",
    "section": "Crude Oil Prices",
    "text": "Crude Oil Prices\n\n\nThe raw data are monthly data starting from January 1986, expressed in dollars per barrel. The cleaned data are monthly series showing the year-over-year changes in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#import-price-index",
    "href": "02_02_data_gathering_and_cleaning.html#import-price-index",
    "title": "Data Gathering and Data Cleaning",
    "section": "Import Price Index",
    "text": "Import Price Index\n\n\nThe raw data are monthly data starting from September 1982, expressed as an index with the 2000 average set to 100. The cleaned data are monthly series showing the year-over-year changes in percent, calculated using the portion of the data that is continuously available from December 1988 onward."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#labor-productivity",
    "href": "02_02_data_gathering_and_cleaning.html#labor-productivity",
    "title": "Data Gathering and Data Cleaning",
    "section": "Labor Productivity",
    "text": "Labor Productivity\n\n\nThe raw data are quarterly series starting from the first quarter of 1947, expressed as an index with the 2017 average set to 100. The cleaned data are series converted into monthly frequency by first expressing the quarter-to-quarter changes at an annualized rate in percent and then applying spline interpolation."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#average-hourly-earnings",
    "href": "02_02_data_gathering_and_cleaning.html#average-hourly-earnings",
    "title": "Data Gathering and Data Cleaning",
    "section": "Average Hourly Earnings",
    "text": "Average Hourly Earnings\n\n\nThe raw data are monthly series starting from March 2006, expressed in dollars per hour. The cleaned data are monthly series showing the month-over-month changes at an annualized rate in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#labor-force-participation-rate",
    "href": "02_02_data_gathering_and_cleaning.html#labor-force-participation-rate",
    "title": "Data Gathering and Data Cleaning",
    "section": "Labor Force Participation Rate",
    "text": "Labor Force Participation Rate\n\n\nThe raw data are monthly series starting from January 1948, expressed in percent. The cleaned data use these values directly; therefore, the two represent the same data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#job-openings",
    "href": "02_02_data_gathering_and_cleaning.html#job-openings",
    "title": "Data Gathering and Data Cleaning",
    "section": "Job Openings",
    "text": "Job Openings\n\n\nThe raw data are monthly series starting from December 2000, expressed in level in thousands. The cleaned data are monthly series showing the month-over-month changes at an annualized rate in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#federal-funds-effective-rate",
    "href": "02_02_data_gathering_and_cleaning.html#federal-funds-effective-rate",
    "title": "Data Gathering and Data Cleaning",
    "section": "Federal Funds Effective Rate",
    "text": "Federal Funds Effective Rate\n\n\nThe raw data are monthly series starting from July 1954, expressed in percent. The cleaned data use these values directly; therefore, the two represent the same data."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#money-suppley-m2",
    "href": "02_02_data_gathering_and_cleaning.html#money-suppley-m2",
    "title": "Data Gathering and Data Cleaning",
    "section": "Money Suppley (M2)",
    "text": "Money Suppley (M2)\n\n\nThe raw data are monthly series starting from January 1959, expressed in billions of dollars. The cleaned data are monthly series showing the month-over-month changes at an annualized rate in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#total-assets-of-federal-reserve",
    "href": "02_02_data_gathering_and_cleaning.html#total-assets-of-federal-reserve",
    "title": "Data Gathering and Data Cleaning",
    "section": "Total Assets of Federal Reserve",
    "text": "Total Assets of Federal Reserve\n\n\nThe raw data are weekly series starting from December 18, 2002, expressed in millions of U.S. dollars. The cleaned data are monthly series showing the year-over-year changes in the monthly averages, expressed in percent."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#year-breakeven-inflation-rate",
    "href": "02_02_data_gathering_and_cleaning.html#year-breakeven-inflation-rate",
    "title": "Data Gathering and Data Cleaning",
    "section": "5-Year Breakeven Inflation Rate",
    "text": "5-Year Breakeven Inflation Rate\n\n\nThe raw data are daily series starting from January 2, 2003, expressed in percent. The cleaned data are series of their monthly averages."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#year-breakeven-inflation-rate-1",
    "href": "02_02_data_gathering_and_cleaning.html#year-breakeven-inflation-rate-1",
    "title": "Data Gathering and Data Cleaning",
    "section": "10-Year Breakeven Inflation Rate",
    "text": "10-Year Breakeven Inflation Rate\n\n\nThe raw data are daily series starting from January 2, 2003, expressed in percent. The cleaned data are series of their monthly averages."
  },
  {
    "objectID": "02_02_data_gathering_and_cleaning.html#year-and-5-year-expected-changes-in-inflation-rates",
    "href": "02_02_data_gathering_and_cleaning.html#year-and-5-year-expected-changes-in-inflation-rates",
    "title": "Data Gathering and Data Cleaning",
    "section": "1-Year and 5-Year Expected Changes in Inflation Rates",
    "text": "1-Year and 5-Year Expected Changes in Inflation Rates\n\n\nThe raw data are monthly series starting from January 1978, expressed in percent. The cleaned data extract the portion from April 1990 onward, for which continuous monthly data are available."
  },
  {
    "objectID": "03_clustering.html",
    "href": "03_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Notes: All images displayed on this page are free-use materials, unless otherwise indicated by a specific credit."
  },
  {
    "objectID": "03_clustering.html#silhouette-scores-of-k-means-and-hierarchical-clustering",
    "href": "03_clustering.html#silhouette-scores-of-k-means-and-hierarchical-clustering",
    "title": "Clustering",
    "section": "Silhouette Scores of K-Means and Hierarchical Clustering",
    "text": "Silhouette Scores of K-Means and Hierarchical Clustering\n\n\nThe silhouette analysis of K-Means clustering (Euclidean distance) shows that the silhouette scores remain relatively modest across all values of k, peaking at about 0.27 when k = 12. This suggests that while K-Means does identify some meaningful separation at this cluster size, the overall cohesion within clusters and separation between clusters is limited. In other words, the structure captured by K-Means is relatively weak, and the method may be less effective at uncovering clear macroeconomic regimes in this dataset.\nIn contrast, hierarchical clustering with cosine distance yields consistently higher silhouette scores, reaching about 0.42 for k = 11. This indicates that hierarchical clustering provides more coherent and well-separated clusters, making it more reliable for identifying distinct macroeconomic states.\nDespite these differences, both methods converge on a similar conclusion: around 11–12 clusters represent a reasonable range for capturing the underlying structure of the data. This agreement suggests that the existence of multiple macroeconomic regimes is robust to the choice of clustering method.\nTaken together, both methods point to 11–12 clusters as a reasonable range, but hierarchical clustering is clearly the stronger approach in this application.\nIn the figures below, the left side shows the results of K-Means Clustering, while the right side shows the results of Hierarchical Clustering.\n\n\n\n\n\n\n\n\nSilhouette scores of K-Means clustering (Euclidean distance) across different numbers of clusters (k = 2–19). The scores peak at k = 12 (≈0.28), suggesting that around 12 clusters provide the most coherent separation under this method.\n\n\n\n\n\n\n\n\n\n\nSilhouette scores of hierarchical clustering with cosine distance across different numbers of clusters (k = 2–19). The scores reach their highest values at k = 11 (≈0.42), indicating stronger cohesion and separation compared to K-Means."
  },
  {
    "objectID": "03_clustering.html#hierarchical-clustering-dendrogram",
    "href": "03_clustering.html#hierarchical-clustering-dendrogram",
    "title": "Clustering",
    "section": "Hierarchical Clustering Dendrogram",
    "text": "Hierarchical Clustering Dendrogram\n\n\nThe figure below is a hierarchical clustering dendrogram based on cosine distance and average linkage. The vertical axis indicates the distance at which clusters merge, with lower values representing greater similarity. Cutting the tree around a distance of 0.6 yields approximately 11–12 distinct clusters, corresponding to different macroeconomic regimes such as normal periods, recessions, and shock episodes.\n\n\n\n\n\n\nHierarchical clustering dendrogram using cosine distance and average linkage. The vertical axis indicates the distance at which clusters merge, with lower values representing greater similarity. Cutting the dendrogram around a distance of 0.6 suggests approximately 11–12 distinct clusters, corresponding to different macroeconomic regimes."
  },
  {
    "objectID": "03_clustering.html#related-to-q1-out-of-10-research-questions-in-introduction",
    "href": "03_clustering.html#related-to-q1-out-of-10-research-questions-in-introduction",
    "title": "Clustering",
    "section": "Related to Q1 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q1 (out of 10 Research Questions) in Introduction\n\nIs a short-run trade-off between inflation and unemployment statistically observable?\n\nYes, it is. In both figures (2D scatterplots of unemployment rate UNRATE and inflation rate PCEPI YoY), periods of higher inflation coincide with lower unemployment clusters (upper-left), while periods of higher unemployment coincide with lower or even negative inflation clusters (lower-right).\nAlthough the relationship is not a perfect straight line, the presence of distinct clusters indicates that the short-run Phillips curve trade-off is statistically observable.\nIn the figures below, the left side shows the results of K-Means Clustering, while the right side shows the results of Hierarchical Clustering.\n\n\n\n\n\n\n\n\nK-Means clustering results with k = 12 on standardized features. The scatter plot shows unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis), with points colored by cluster assignment. Distinct groupings illustrate the short-run Phillips curve trade-off, where clusters with higher inflation correspond to lower unemployment and vice versa.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results with k = 11 on standardized features. The scatter plot shows unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis), with points colored by cluster assignment. The clusters reveal distinct groupings that highlight the short-run Phillips curve trade-off, where periods of higher inflation are generally associated with lower unemployment, and vice versa."
  },
  {
    "objectID": "03_clustering.html#related-to-q2-out-of-10-research-questions-in-introduction",
    "href": "03_clustering.html#related-to-q2-out-of-10-research-questions-in-introduction",
    "title": "Clustering",
    "section": "Related to Q2 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q2 (out of 10 Research Questions) in Introduction\n\nTo what extent does the Phillips curve relationship vary across business-cycle phases (recessions and expansions) and policy regimes (e.g., the zero lower bound period or the COVID-19 era)?\n\nBy conducting clustering based on whether the economy was in a recession or not (USREC = 0/1), during the Zero Lower Bound (ZLB) period or not (ZLB_dummy = 0/1), and in the COVID-19 era or not (COVID_dummy = 0/1), the negative relationship between unemployment and inflation was generally preserved in all cases. In other words, although the slope varied in strength across different economic phases and policy regimes, the basic Phillips curve relationship consistently held.\nIn the figures below, the left sides show the results of K-Means Clustering, while the right sides show the results of Hierarchical Clustering.\n\n\n\nU.S. Recession\n\nNon-Recession Period (USREC = 0)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-recession periods (USREC = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 9. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside recessions.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-recession periods (USREC = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 10. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside recessions.\n\n\n\n\n\n\n\nRecession Period (USREC = 1)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during recession periods (USREC = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 3. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved within recessions.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during recession periods (USREC = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 2. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved within recessions.\n\n\n\n\n\n\n\n\nZero Lower Bound Period\n\nNon-Zero Lower Bound Period (ZLB_dummy = 0)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-zero lower bound periods (ZLB_dummy = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 7. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside zero lower bound periods.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-zero lower bound periods (ZLB_dummy = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 3. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside zero lower bound periods.\n\n\n\n\n\n\n\nZero Lower Bound Period (ZLB_dummy = 1)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during zero lower bound periods (ZLB_dummy = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 5. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved during zero lower bound periods.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during zero lower bound periods (ZLB_dummy = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 13. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved during zero lower bound periods.\n\n\n\n\n\n\n\n\nCOVID-19 Era\n\nNon-COVID-19 Era (COVID_dummy = 0)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-COVID-19 era (COVID_dummy = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 10. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside COVID-19 era.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during non-COVID-19 era (COVID_dummy = 0). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 4. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved outside COVID-19 era.\n\n\n\n\n\n\n\nCOVID-19 Bound Era (COVID_dummy = 1)\n\n\n\n\n\n\nK-Means clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during COVID-19 era (COVID_dummy = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 2. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved during COVID-19 era.\n\n\n\n\n\n\n\n\n\n\nHierarchical clustering results on standardized unemployment (UNRATE, x-axis) and inflation (PCEPILFE_YoY, y-axis) during COVID-19 era (COVID_dummy = 1). Based on silhouette analysis conducted for this subset, the optimal number of clusters was determined to be k = 6. The resulting clusters reveal distinct macroeconomic groupings, indicating that the short-run Phillips curve relationship is generally preserved during COVID-19 era."
  },
  {
    "objectID": "03_clustering.html#related-to-q4-out-of-10-research-questions-in-introduction",
    "href": "03_clustering.html#related-to-q4-out-of-10-research-questions-in-introduction",
    "title": "Clustering",
    "section": "Related to Q4 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q4 (out of 10 Research Questions) in Introduction\n\nDoes regime identification based on data-driven clustering align with historical episodes such as stagflation or the post–global financial crisis recovery?\n\nYes, it does. In both K-Means clustering and hierarchical clustering, each cluster tends to be strongly skewed toward either 0 or 1 for the dummy variables representing recessions (USREC), the zero lower bound period (ZLB_dummy), and the COVID era (COVID_dummy), suggesting the effectiveness of clustering in identifying regimes.\n\n\n\nK-Means Clustering\n\n\nIn K-Means clustering, each cluster tends to be strongly skewed toward either 0 or 1 for the dummy variables representing recessions (USREC), the zero lower bound period (ZLB_dummy), and the COVID era (COVID_dummy), suggesting the effectiveness of clustering in identifying regimes. For example, focusing on Cluster 0, since USREC is entirely 0, ZLB_dummy is entirely 1, and COVID_dummy is entirely 0, it can be seen that the data classified into this cluster correspond to periods that are not recessions, are within the zero lower bound period, and are not during the COVID era, thereby allowing regimes to be clearly identified. For other clusters as well, although in some cases the proportions of 0 and 1 for certain dummy variables are nearly balanced, making regime identification more difficult, in general the regimes can be clearly distinguished.\n\n\n\n\n\n\nDistribution of dummy variables within clusters from K-Means clustering (k = 12). Each donut chart shows the share of 0 vs. 1 for USREC (recessions), ZLB_dummy (zero lower bound period), and COVID_dummy (COVID-19 era) across clusters. Most clusters are strongly skewed toward either 0 or 1, indicating that K-Means effectively distinguishes macroeconomic regimes such as normal periods, recessions, the ZLB period, and the COVID-19 shock.\n\n\n\n\n\nHierarchical Clustering\n\n\nIn hierarchical clustering, each cluster tends to be strongly skewed toward either 0 or 1 for the dummy variables representing recessions (USREC), the zero lower bound period (ZLB_dummy), and the COVID era (COVID_dummy), suggesting the effectiveness of clustering in identifying regimes. For example, focusing on Cluster 1, since USREC, ZLB_dummy, and COVID_dummy are all 0, it can be seen that the data classified into this cluster correspond to periods that are not recessions, are not within the zero lower bound period, and are not during the COVID era, thereby allowing regimes to be clearly identified. For other clusters as well, although in some cases the proportions of 0 and 1 for certain dummy variables are nearly balanced, making regime identification more difficult, in general the regimes can be clearly distinguished.\n\n\n\n\n\n\nDistribution of dummy variables within clusters from hierarchical clustering (k = 11). Each donut chart shows the share of 0 vs. 1 for USREC (recessions), ZLB_dummy (zero lower bound period), and COVID_dummy (COVID-19 era) across clusters. Many clusters are strongly skewed toward either 0 or 1, indicating that hierarchical clustering effectively separates macroeconomic regimes such as normal periods, recessions, ZLB episodes, and the COVID-19 shock."
  },
  {
    "objectID": "05_naive_bayes.html",
    "href": "05_naive_bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naïve Bayes is a simple yet powerful probabilistic classification algorithm based on Bayes’ Theorem. It performs especially well in tasks like text classification (e.g., spam filters) and medical diagnosis.\nThe word “naïve” in its name comes from the very simple, yet powerful, assumption the algorithm makes: that all features are independent of one another.\nFor example, if an email contains the words “congratulations” and “free,” Naïve Bayes treats the probability of “congratulations” appearing and the probability of “free” appearing as two separate, independent events that do not influence each other. In reality, these words are often related, but this “naïve assumption” greatly simplifies the computation and works very effectively in many cases.\n\n\n\n\n\n\n\n\nThis image shows how the Naïve Bayes classifier uses Bayes’ Theorem (top) with a “naïve” assumption (bottom) that all features are independent, simplifying the calculation. Source: UC Business Analytics R Programming Guide\n\n\n\n\n\n\n\n\nThis diagram shows how a spam filter works. A classifier, often powered by an algorithm like Naïve Bayes, analyzes incoming emails and automatically sorts them into the INBOX for legitimate mail and the SPAM FOLDER for junk mail. Source: Google Developers\n\n\n\n\n\n\n\n\n\n\nMultinomial Naïve Bayes is a model that is particularly effective when features represent counts or frequencies. Text classification is a classic example.\nDefinition: It assumes that each feature (e.g., each word in a document) is drawn from a multinomial distribution. This is a model for counting how many times an event occurred.\nExplanation: Taking email classification as an example, Multinomial NB calculates the probability that an email is spam based on the frequency of words like “congratulations,” “sale,” or “urgent.” The feature values are discrete counts, such as 0, 1, 2, 3, and so on.\n\n\nUse Cases:\n\nSpam Filtering: Classifying an email as spam or not based on the frequency of words it contains.\nDocument Classification: Categorizing news articles into topics like “sports,” “business,” or “politics.”\n\n\n\n\n\n\n\n\nBernoulli Naïve Bayes is a model used when features are binary (i.e., they take one of two values, like “yes” or “no”).\nDefinition: It assumes that each feature is drawn from a Bernoulli distribution (a trial with only two outcomes, like a coin flip).\nExplanation: Using text classification as an example again, while Multinomial NB counts word occurrences, Bernoulli NB only considers the presence (1) or absence (0) of a specific word in a document. Whether a word appears 10 times or just once, Bernoulli NB treats it as “1” (present).\n\n\nUse Cases:\n\nText Classification: Classifying documents based on whether or not they contain specific keywords.\nSentiment Analysis: Determining if a review is positive or negative based on the presence of words like “excellent” or “terrible.”"
  },
  {
    "objectID": "05_naive_bayes.html#the-naïve-bayes-nb-classifier",
    "href": "05_naive_bayes.html#the-naïve-bayes-nb-classifier",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naïve Bayes is a simple yet powerful probabilistic classification algorithm based on Bayes’ Theorem. It performs especially well in tasks like text classification (e.g., spam filters) and medical diagnosis.\nThe word “naïve” in its name comes from the very simple, yet powerful, assumption the algorithm makes: that all features are independent of one another.\nFor example, if an email contains the words “congratulations” and “free,” Naïve Bayes treats the probability of “congratulations” appearing and the probability of “free” appearing as two separate, independent events that do not influence each other. In reality, these words are often related, but this “naïve assumption” greatly simplifies the computation and works very effectively in many cases.\n\n\n\n\n\n\n\n\nThis image shows how the Naïve Bayes classifier uses Bayes’ Theorem (top) with a “naïve” assumption (bottom) that all features are independent, simplifying the calculation. Source: UC Business Analytics R Programming Guide\n\n\n\n\n\n\n\n\nThis diagram shows how a spam filter works. A classifier, often powered by an algorithm like Naïve Bayes, analyzes incoming emails and automatically sorts them into the INBOX for legitimate mail and the SPAM FOLDER for junk mail. Source: Google Developers"
  },
  {
    "objectID": "05_naive_bayes.html#multinomial-naïve-bayes",
    "href": "05_naive_bayes.html#multinomial-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Multinomial Naïve Bayes is a model that is particularly effective when features represent counts or frequencies. Text classification is a classic example.\nDefinition: It assumes that each feature (e.g., each word in a document) is drawn from a multinomial distribution. This is a model for counting how many times an event occurred.\nExplanation: Taking email classification as an example, Multinomial NB calculates the probability that an email is spam based on the frequency of words like “congratulations,” “sale,” or “urgent.” The feature values are discrete counts, such as 0, 1, 2, 3, and so on.\n\n\nUse Cases:\n\nSpam Filtering: Classifying an email as spam or not based on the frequency of words it contains.\nDocument Classification: Categorizing news articles into topics like “sports,” “business,” or “politics.”"
  },
  {
    "objectID": "05_naive_bayes.html#bernoulli-naïve-bayes",
    "href": "05_naive_bayes.html#bernoulli-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Bernoulli Naïve Bayes is a model used when features are binary (i.e., they take one of two values, like “yes” or “no”).\nDefinition: It assumes that each feature is drawn from a Bernoulli distribution (a trial with only two outcomes, like a coin flip).\nExplanation: Using text classification as an example again, while Multinomial NB counts word occurrences, Bernoulli NB only considers the presence (1) or absence (0) of a specific word in a document. Whether a word appears 10 times or just once, Bernoulli NB treats it as “1” (present).\n\n\nUse Cases:\n\nText Classification: Classifying documents based on whether or not they contain specific keywords.\nSentiment Analysis: Determining if a review is positive or negative based on the presence of words like “excellent” or “terrible.”"
  },
  {
    "objectID": "05_naive_bayes.html#training-and-test-data-for-yoy-inflation-prediction-high_inflation_yoy-derived-from-pcepilfe_yoy",
    "href": "05_naive_bayes.html#training-and-test-data-for-yoy-inflation-prediction-high_inflation_yoy-derived-from-pcepilfe_yoy",
    "title": "Naïve Bayes",
    "section": "Training and Test Data for YoY Inflation Prediction (high_inflation_YoY derived from PCEPILFE_YoY)",
    "text": "Training and Test Data for YoY Inflation Prediction (high_inflation_YoY derived from PCEPILFE_YoY)\n\n\nFeature Engineering: Lag variables (from 1 to 13 months prior) were added for each economic indicator to expand the feature set.\nDefining Predictors and Target: The binarized YoY inflation rate (high_inflation_YoY) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The Training Data for YoY Inflation\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The Test Data for YoY Inflation"
  },
  {
    "objectID": "05_naive_bayes.html#training-and-test-data-for-mom-inflation-prediction-high_inflation_mom-derived-from-pcepilfe_mom",
    "href": "05_naive_bayes.html#training-and-test-data-for-mom-inflation-prediction-high_inflation_mom-derived-from-pcepilfe_mom",
    "title": "Naïve Bayes",
    "section": "Training and Test Data for MoM Inflation Prediction (high_inflation_MoM derived from PCEPILFE_MoM)",
    "text": "Training and Test Data for MoM Inflation Prediction (high_inflation_MoM derived from PCEPILFE_MoM)\n\n\nFeature Engineering: To capture past trends and volatility, 8-month rolling window statistics (e.g., mean, standard deviation) were calculated for key economic indicators and added as features. For this specific model, no lag variables were used.\nDefining Predictors and Target: The binarized MoM inflation rate (high_inflation_MoM) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\nOversampling the Training Data: To address the difficulty of classifying the “High Inflation” periods, the training data was resampled using SMOTE. A custom sampling strategy was employed to oversample the minority class to be twice the size of the majority class, compelling the model to learn its characteristics more thoroughly. This resampling was applied only to the training set.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The Training Data for MoM Inflation\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The Test Data for MoM Inflation"
  },
  {
    "objectID": "05_naive_bayes.html#training-and-test-data-for-unemployment-rates-high_unemployment-derived-from-unrate",
    "href": "05_naive_bayes.html#training-and-test-data-for-unemployment-rates-high_unemployment-derived-from-unrate",
    "title": "Naïve Bayes",
    "section": "Training and Test Data for Unemployment Rates (high_unemployment derived from UNRATE)",
    "text": "Training and Test Data for Unemployment Rates (high_unemployment derived from UNRATE)\n\n\nFeature Engineering: To capture past trends and volatility, 12-month rolling window statistics (e.g., mean, standard deviation) were calculated for key economic indicators and added as features. For this specific model, no lag variables were used.\nDefining Predictors and Target: The binarized unemployment rate (high_unemployment) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.\nData Splitting: The dataset was divided into a 70% training set and a 30% testing set using stratified sampling to ensure the class proportions were identical in both sets.\nSelective Scaling: Only continuous variables were standardized using StandardScaler, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: The Training Data for Unemployment Rates\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: The Test Data for Unemployment Rates"
  },
  {
    "objectID": "05_naive_bayes.html#confusion-matrix-for-yoy-inflation-prediction",
    "href": "05_naive_bayes.html#confusion-matrix-for-yoy-inflation-prediction",
    "title": "Naïve Bayes",
    "section": "Confusion Matrix for YoY Inflation Prediction",
    "text": "Confusion Matrix for YoY Inflation Prediction\n\n\nAccuracy: 86.4%\nThis is a strong and well-balanced model. Its key strength is a very high recall for “Low Inflation” (94%), meaning it is excellent at correctly identifying low inflation periods.\nThe model is also very reliable when it predicts high inflation (precision of 92%). Its main weakness is a slightly lower recall for “High Inflation” (77%), indicating it occasionally misses identifying a high inflation period. Overall, its performance is robust.\n\n\n\n\n\n\n\n\n\nFigure 7: Confusion Matrix for YoY Inflation Prediction"
  },
  {
    "objectID": "05_naive_bayes.html#confusion-matrix-for-mom-inflation-prediction",
    "href": "05_naive_bayes.html#confusion-matrix-for-mom-inflation-prediction",
    "title": "Naïve Bayes",
    "section": "Confusion Matrix for MoM Inflation Prediction",
    "text": "Confusion Matrix for MoM Inflation Prediction\n\n\nAccuracy: 70.6%\nThis model’s performance is mixed. Its main strength is a high recall (86%) for “Low Inflation,” meaning it is effective at identifying low inflation periods.\nHowever, its primary weakness is a low recall (55%) for the “High Inflation” class, indicating that it fails to detect a significant portion (45%) of the actual high inflation periods. While its precision for predicting “High Inflation” is solid (78%), the model is clearly more skilled at identifying low inflation than high inflation.\n\n\n\n\n\n\n\n\n\nFigure 8: Confusion Matrix for MoM Inflation Prediction"
  },
  {
    "objectID": "05_naive_bayes.html#confusion-matrix-for-unemployment-rates-prediction",
    "href": "05_naive_bayes.html#confusion-matrix-for-unemployment-rates-prediction",
    "title": "Naïve Bayes",
    "section": "Confusion Matrix for Unemployment Rates Prediction",
    "text": "Confusion Matrix for Unemployment Rates Prediction\n\n\nAccuracy: 86.4%\nThis is a strong and reliable model. Its most notable strengths are its perfect recall (100%) for “Low Unemployment” and its perfect precision (100%) for “High Unemployment.” This means the model never misclassifies a low unemployment period and its predictions of high unemployment are never wrong.\nThe model’s main weakness is a good but not perfect recall for “High Unemployment” (72%), indicating that it fails to detect more than a quarter (~28%) of the high unemployment periods. Overall, despite this, the model is highly accurate and its predictions are very trustworthy.\n\n\n\n\n\n\n\n\n\nFigure 9: Confusion Matrix for Unemployment Rates Prediction"
  },
  {
    "objectID": "05_naive_bayes.html#related-to-q5-out-of-10-research-questions-in-introduction",
    "href": "05_naive_bayes.html#related-to-q5-out-of-10-research-questions-in-introduction",
    "title": "Naïve Bayes",
    "section": "Related to Q5 (out of 10 Research Questions) in Introduction",
    "text": "Related to Q5 (out of 10 Research Questions) in Introduction\n\n\nRefer to the conclusion below."
  },
  {
    "objectID": "07_svm.html",
    "href": "07_svm.html",
    "title": "Support Vector Machine (SVM)",
    "section": "",
    "text": "Overview\n\nxxx\n\n\n\nData Prep\n\nxxx\n\n\n\nCode\n\nxxx\n\n\n\nResults\n\nxxx\n\n\n\nConclusions\n\nxxx"
  },
  {
    "objectID": "09_nn.html",
    "href": "09_nn.html",
    "title": "Neural Network (NN)",
    "section": "",
    "text": "Overview\n\nxxx\n\n\n\nData Prep\n\nxxx\n\n\n\nCode\n\nxxx\n\n\n\nResults\n\nxxx\n\n\n\nConclusions\n\nxxx"
  }
]