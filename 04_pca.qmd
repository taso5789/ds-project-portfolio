---
title: "Principal Component Analysis (PCA)"
---

::: {.justify}
Notes: All images displayed on this page are free-use materials, unless otherwise indicated by a specific credit.
:::

# Overview

::: {.justify}
- **Principal Component Analysis (PCA)** is a widely used dimensionality reduction technique in data science and statistics. The main idea of PCA is to transform a set of correlated variables into a smaller set of uncorrelated variables, called principal components, while preserving as much of the original information as possible. Each principal component is a linear combination of the original variables and captures the directions of maximum variance in the dataset.

- At the core of PCA are **eigenvalues** and **eigenvectors**. Eigenvectors define the directions (principal components) along which the data varies the most, while eigenvalues indicate the amount of variance explained by each component. The larger the eigenvalue, the more information the component carries. By ranking components according to their eigenvalues, we can determine how many dimensions should be retained without losing significant information.

- **The significance of dimensionality reduction** lies in **the simplification of analysis and visualization**. By reducing the number of dimensions, patterns and groupings become clearer, and the data can be effectively displayed in two or three dimensions. This allows researchers to intuitively explore relationships, clusters, and structures that would otherwise remain hidden in high-dimensional space.

- **High-dimensional data often poses serious challenges**. It may contain redundancy and noise, and as the number of dimensions increases, the data becomes sparse. This phenomenon is known as the curse of dimensionality, which reduces the reliability of distance calculations, clustering, and nearest-neighbor searches. For example, in low-dimensional space, points may appear relatively close to each other, but in high-dimensional space they tend to be almost equally distant, making it difficult to identify meaningful patterns or similarities. **PCA addresses these issues** by reducing the dimensionality of the dataset while retaining the most important sources of variance. By projecting the data onto fewer principal components, PCA removes redundancy, suppresses noise, and produces a more compact and manageable dataset. This not only improves computational efficiency but also enhances the performance of subsequent machine learning algorithms.
:::

::: {.columns}
::: {.column width="50%"}
![](images/fig_04_overview_01.png){fig-align="center" width="100%"}
:::
::: {.column width="50%"}
![](images/fig_04_overview_02.png){fig-align="center" width="100%"}
:::
:::

# Data Prep

::: {.justify}
- **Principal Component Analysis requires numeric and standardized data**. Since PCA is based on variance and covariance, it can only be applied to numerical variables. If the dataset contains categorical or textual features, they must first be converted into numerical form. For example, categorical variables can be transformed using one-hot encoding, while text data can be vectorized with appropriate methods. Moreover, PCA is sensitive to differences in scale, so it is crucial to normalize or standardize the variables before applying the method. For instance, if one feature is measured in thousands and another in decimals, the feature with the larger scale will dominate the variance and bias the results. Standardization—subtracting the mean and dividing by the standard deviation for each variable—ensures that all features contribute equally to the principal components.
:::

::: {.justify}
- The sample image of the data used in the analysis on this page is shown below. This dataset corresponds to the one shown in "Merged Cleaned Data" at the end of [Data Gathering and Data Cleaning](02_02_data_gathering_and_cleaning.qmd).
:::

::: {.text-center}
![](results/3_PCA/data.jpg){width=90%}
:::

- <mark><span style="color:red">Link to Data</span></mark>


# Code

- <mark><span style="color:red">Link to Code</span></mark>

# Results

- xxx

# Conclusions

- xxx