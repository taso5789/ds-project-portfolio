---
title: "Naïve Bayes"
---

# Overview

## The Naïve Bayes (NB) Classifier

::: {.justify}
- Naïve Bayes is a simple yet powerful **probabilistic classification algorithm** based on **Bayes' Theorem**. It performs especially well in tasks like text classification (e.g., spam filters) and medical diagnosis.

- The word "**naïve**" in its name comes from the very simple, yet powerful, assumption the algorithm makes: that **all features are independent of one another**.

- For example, if an email contains the words "congratulations" and "free," Naïve Bayes treats the probability of "congratulations" appearing and the probability of "free" appearing as two separate, independent events that do not influence each other. In reality, these words are often related, but this "naïve assumption" greatly simplifies the computation and works very effectively in many cases.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![This image shows how the Naïve Bayes classifier uses **Bayes' Theorem** (top) with a **"naïve" assumption** (bottom) that all features are independent, simplifying the calculation. Source: [UC Business Analytics R Programming Guide](https://uc-r.github.io/naive_bayes){target="_blank"}](images/fig_05_overview_01.png){fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![This diagram shows how a spam filter works. A **classifier**, often powered by an algorithm like **Naïve Bayes**, analyzes incoming emails and automatically sorts them into the **INBOX** for legitimate mail and the **SPAM FOLDER** for junk mail. Source: [Google Developers](https://developers.google.com/machine-learning/guides/text-classification){target="_blank"}](images/fig_05_overview_02.jpg){fig-align="center" width="100%"}
:::
:::
:::

## Multinomial Naïve Bayes

::: {.justify}
- **Multinomial Naïve Bayes** is a model that is particularly effective when features represent **counts or frequencies**. Text classification is a classic example.

- **Definition:** It assumes that each feature (e.g., each word in a document) is drawn from a multinomial distribution. This is a model for counting how many times an event occurred.
- **Explanation:** Taking email classification as an example, Multinomial NB calculates the probability that an email is spam based on the **frequency of words** like "congratulations," "sale," or "urgent." The feature values are discrete counts, such as 0, 1, 2, 3, and so on.

> **Use Cases:**
>
> - **Spam Filtering:** Classifying an email as spam or not based on the frequency of words it contains.
>
> - **Document Classification:** Categorizing news articles into topics like "sports," "business," or "politics."
:::

## Bernoulli Naïve Bayes

::: {.justify}
- **Bernoulli Naïve Bayes** is a model used when features are **binary** (i.e., they take one of two values, like "yes" or "no").

- **Definition:** It assumes that each feature is drawn from a Bernoulli distribution (a trial with only two outcomes, like a coin flip).
- **Explanation:** Using text classification as an example again, while Multinomial NB counts word *occurrences*, Bernoulli NB only considers the **presence (1) or absence (0)** of a specific word in a document. Whether a word appears 10 times or just once, Bernoulli NB treats it as "1" (present).

> **Use Cases:**
>
> - **Text Classification:** Classifying documents based on whether or not they contain specific keywords.
>
> - **Sentiment Analysis:** Determining if a review is positive or negative based on the presence of words like "excellent" or "terrible."
:::

# Data Prep

::: {.justify}
- The fundamental prerequisite for building a supervised learning model is the use of **"labeled data."** This refers to a dataset where each data point is provided with a "correct answer" or label in advance. For example, this could be email data where each message is labeled as either "Spam" or "Not Spam." The model uses these correct labels as a target to learn patterns within the data.

- Furthermore, to fairly evaluate the model's performance on new, unseen data, the dataset is split into a **Training Set** and a **Testing Set**. This division prevents **"overfitting,"** where the model simply memorizes the training data, and allows for a proper measurement of its ability to generalize.

  - **Training Set**: The majority of the dataset, used for the model to learn the patterns and relationships in the data.
  - **Testing Set**: A portion of the data held back from the training process, used only to verify the final performance of the trained model.

- The training and testing data must be **disjoint** sets, meaning they have no data points in common. The reason is to get an honest evaluation of the model's ability to generalize to new, unseen data. If the sets overlap, the model is being tested on data it has already "seen" during training, which would be like giving a student an exam with questions they had already memorized. Keeping the training and testing sets **disjoint** is the only way to get a true measure of the model's predictive performance.

- The sample image of the data used in the analysis on this page is shown below. This dataset corresponds to the one shown in "Merged Cleaned Data" at the end of [Data Gathering and Data Cleaning](02_02_data_gathering_and_cleaning.qmd). The dataset can be found [here](https://github.com/taso5789/ds-project-portfolio/tree/main/data/df_merged.csv).
  - For this analysis, the target variables—inflation rates (`PCEPILFE_YoY`, `PCEPILFE_MoM`) and the unemployment rate (`UNRATE`)—were binarized to apply the Naïve Bayes classifier. The historical median of each variable was used as a threshold; periods with values above the median were defined as "High Inflation" or "High Unemployment" (1), and all others as "Low" (0).
:::

::: {.text-center}
![](results/4_Naive_Bayes/data.jpg){width=100%}
:::

::: {.justify}
- The training and testing data used to predict the year-over-year (YoY) inflation (`PCEPILFE_YoY`), month-over-month (MoM) inflation (`PCEPILFE_MoM`), and unemployment rates (`UNRATE`) are shown in @fig-inf-YoY-train ~ @fig-unemp-test, respectively. Each dataset was created as follows:
:::

## Training and Test Data for YoY Inflation Prediction (`high_inflation_YoY` derived from `PCEPILFE_YoY`)

::: {.justify}
1.  **Feature Engineering**: **Lag variables** (from 1 to 13 months prior) were added for each economic indicator to expand the feature set.

2.  **Defining Predictors and Target**:  The binarized YoY inflation rate (`high_inflation_YoY`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for YoY Inflation](results/4_Naive_Bayes/inflation_YoY_train_data.jpg){#fig-inf-YoY-train}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for YoY Inflation](results/4_Naive_Bayes/inflation_YoY_test_data.jpg){#fig-inf-YoY-test}
:::
:::
:::

## Training and Test Data for MoM Inflation Prediction (`high_inflation_MoM` derived from `PCEPILFE_MoM`)

::: {.justify}
1.  **Feature Engineering**: To capture recent trends and volatility, **6-month rolling window statistics** (e.g., mean, standard deviation) were calculated for key economic indicators and added as features. For this specific model, no lag variables were used.

2.  **Defining Predictors and Target**: The binarized MoM inflation rate (`high_inflation_MoM`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.

5.  **Oversampling the Training Data**: To address the difficulty of classifying the "High Inflation" periods, the training data was resampled using **SMOTE**. A custom sampling strategy was employed to oversample the minority class to be **twice the size of the majority class**, compelling the model to learn its characteristics more thoroughly. This resampling was applied *only* to the training set.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for MoM Inflation](results/4_Naive_Bayes/inflation_MoM_train_data.jpg){#fig-inf-MoM-train fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for MoM Inflation](results/4_Naive_Bayes/inflation_MoM_test_data.jpg){#fig-inf-MoM-test fig-align="center" width="100%"}
:::
:::
:::

## Training and Test Data for Unemployment Rates (`high_unemployment` derived from `UNRATE`)

::: {.justify}
1.  **Feature Engineering**: To provide the model with historical context, the feature set was expanded. Specifically, **3-month lag variables** and **12-month rolling window statistics** (e.g., mean, standard deviation) were calculated for each economic indicator and added as features.

2.  **Defining Predictors and Target**: The binarized unemployment rate (`high_unemployment`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for Unemployment Rates](results/4_Naive_Bayes/unemp_train_data.jpg){#fig-unemp-train fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for Unemployment Rates](results/4_Naive_Bayes/unemp_test_data.jpg){#fig-unemp-test fig-align="center" width="100%"}
:::
:::
:::

# Code

::: {.justify}
- For the code used in this chapter (Naïve Bayes), please refer [here](https://github.com/taso5789/ds-project-portfolio/blob/main/code/4_Naive_Bayes.ipynb).
:::

# Results

## Confusion Matrix for YoY Inflation Prediction

::: {.justify}
- **Accuracy**: **86.4%**

- This is a **strong and well-balanced model**. Its key strength is a very high **recall for "Low Inflation" (94%)**, meaning it is excellent at correctly identifying low inflation periods.

- The model is also very reliable when it predicts high inflation (**precision of 92%**). Its main weakness is a slightly lower **recall for "High Inflation" (77%)**, indicating it occasionally misses identifying a high inflation period. Overall, its performance is robust.
:::

::: {.justify}
![Confusion Matrix for YoY Inflation Prediction](results/4_Naive_Bayes/inflation_YoY_confusion_matrix.jpg){#fig-inf-YoY-matrix fig-align="center" width="100%"}
:::

## Confusion Matrix for MoM Inflation Prediction

::: {.justify}
- **Accuracy**: **70.6%**

- This model's performance is mixed. Its main strength is a high **recall (86%)** for "Low Inflation," meaning it is effective at identifying low inflation periods.

- However, its primary weakness is a low **recall (55%)** for the "High Inflation" class, indicating that it fails to detect a significant portion (45%) of the actual high inflation periods. While its precision for predicting "High Inflation" is solid (**78%**), the model is clearly more skilled at identifying low inflation than high inflation.
:::

::: {.justify}
![Confusion Matrix for MoM Inflation Prediction](results/4_Naive_Bayes/inflation_MoM_confusion_matrix.jpg){#fig-inf-MoM-matrix fig-align="center" width="100%"}
:::

## Confusion Matrix for Unemployment Rates Prediction

::: {.justify}
- **Accuracy**: **86.4%**

- This is a **strong and reliable model**. Its most notable strengths are its **perfect recall (100%)** for "Low Unemployment" and its **perfect precision (100%)** for "High Unemployment." This means the model never misclassifies a low unemployment period and its predictions of high unemployment are never wrong.

- The model's main weakness is a good but not perfect **recall for "High Unemployment" (72%)**, indicating that it fails to detect more than a quarter (~28%) of the high unemployment periods. Overall, despite this, the model is highly accurate and its predictions are very trustworthy.
:::

::: {.justify}
![Confusion Matrix for Unemployment Rates Prediction](results/4_Naive_Bayes/unemp_confusion_matrix.jpg){#fig-unemp-matrix fig-align="center" width="100%"}
:::

## Related to Q5 (out of 10 Research Questions) in [Introduction](01_introduction.qmd)

::: {.justify}
- Refer to the conclusion below.
:::

# Conclusions

::: {.justify}
- The results of this analysis clearly demonstrate that the performance of the Naïve Bayes model is **extremely dependent on the quality of feature engineering and data preprocessing**.

  1.  **Dramatic Performance Improvement via Preprocessing**: The initial, simple model showed low accuracy and a tendency to miss important periods, especially for predicting month-over-month (MoM) inflation. However, after applying advanced preprocessing techniques—including **feature engineering with lag variables and rolling statistics**, as well as **oversampling with SMOTE**—the same Naïve Bayes model achieved a **remarkable improvement in performance**.

  2.  **Final High Predictive Accuracy**: Ultimately, the model achieved strong and comparable accuracy scores of **86.4% for both unemployment and year-over-year (YoY) inflation prediction**. The unemployment model was particularly noteworthy, demonstrating exceptional reliability by achieving **perfect precision (100%)** for its "high unemployment" predictions and **perfect recall (100%)** for "low unemployment" periods.

  3.  **Remaining Challenges**: On the other hand, predicting the noisy, short-term MoM inflation rate remained a challenge (with 70.6% accuracy), suggesting limitations stemming from the model's fundamental assumption of feature independence.

- **In summary**, while Naïve Bayes is often considered a simple baseline, this analysis concludes that **it can be transformed into a powerful and high-accuracy tool for identifying macroeconomic regimes, provided it is supported by robust feature engineering and data preprocessing**. The results confirm that the quality of the input features was a more critical factor in determining predictive accuracy than the initial complexity of the model.
:::
