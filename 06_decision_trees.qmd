---
title: "Decision Trees"
---

# Overview

## Decision Trees (DTs)

::: {.justify}
- A **Decision Tree** is a supervised learning model that uses a **tree-like structure similar to a flowchart** to classify or regress data. It arrives at a final prediction by repeatedly asking a series of "if-then-else" questions. A major advantage of this model is that its results are very easy to interpret due to its intuitive structure.

- A decision tree is composed of the following elements:
  -  **Root Node:** The starting point of the tree, representing the first question.
  -  **Decision Node:** An intermediate node that represents an intermediate question used to further split the data.
  -  **Leaf Node (or Terminal Node):** The endpoint of a branch, representing the final predicted outcome (a class label or a numerical value).

- This model is utilized in a wide range of fields, including customer purchase prediction, medical diagnosis, and credit scoring.
:::

::: {.justify}
::: {.columns}
::: {.column width="60%"}
![A visual comparison of machine learning models. **Supervised learning** (left), which includes algorithms like **Decision Trees**, uses labeled data to classify a new point. In contrast, **unsupervised learning** (right) finds hidden clusters in unlabeled data. Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Supervised_and_unsupervised_learning.png){target="_blank"}](images/fig_06_overview_01.png){fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="35%"}
![A decision tree model predicting Titanic passenger survival. The structure flows from the initial **root node** ('gender'), through intermediate **decision nodes** ('age,' 'sibsp'), to the final predictions in the **leaf nodes** ('survived'/'died'). Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Decision_Tree.jpg){target="_blank"}](images/fig_06_overview_02.jpg){fig-align="center" width="100%"}
:::
:::
:::

## Gini Impurity, Entropy, and Information Gain

::: {.justify}
- At each step, a decision tree must determine **"which question will most efficiently split the data."** To make this judgment, it uses metrics like **Gini Impurity** or **Entropy**. These metrics measure the **"impurity"** or **"disorder"** of the data within a node.

-  **Gini Impurity:** This metric indicates how mixed the classes are within a node. A value close to 0 means the node is "pure" (contains data from mostly one class), while a higher value means it is "impure" (contains a mix of different classes). It is known for being relatively fast to compute.
    $$
    Gini(S) = 1 - \sum_{i=1}^{c} (p_i)^2
    $$
    where $p_i$ is the proportion of data belonging to class $i$.

-  **Entropy:** Originating from information theory, this is another metric for measuring data impurity. Like Gini Impurity, a value of 0 indicates perfect purity, while a higher value indicates more impurity.
    $$
    H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)
    $$

- **Information Gain** is the metric that shows **"how much the impurity decreased"** as a result of splitting the data by a certain question (feature). At each step, the decision tree selects the question that **maximizes this Information Gain**.

- Information Gain is calculated with the following formula:

  > **Information Gain = Impurity before split – (Weighted average of impurity of each node after split)**
  
  In other words, a split that creates purer groups (groups with lower impurity) is judged to be a "good split."
:::

## A Simple Example of Measuring a Split's "Goodness"

::: {.justify}
- Let's consider a small dataset used to predict "whether to play tennis or not" based on weather data.

  - **Entire Dataset (10 instances):**
      * Play Tennis: 5
      * Don't Play Tennis: 5

### Step 1: Calculate the Gini Impurity Before the Split
- First, we calculate the Gini Impurity of the entire dataset (the root node).

$$Gini_{root} = 1 - ((\frac{5}{10})^2 + (\frac{5}{10})^2) = 1 - (0.25 + 0.25) = \mathbf{0.5}$$

### Step 2: Try Splitting by "Wind"
- Now, let's assume we split the data based on whether the "Wind is Weak or Strong."
 
  - **Weak Wind Group (6 instances):**
      * Play Tennis: 5
      * Don't Play Tennis: 1
      * $Gini_{weak} = 1 - ((\frac{5}{6})^2 + (\frac{1}{6})^2) \approx 1 - (0.694 + 0.028) = \mathbf{0.278}$
  
  - **Strong Wind Group (4 instances):**
      * Play Tennis: 0
      * Don't Play Tennis: 4
      * $Gini_{strong} = 1 - ((\frac{0}{4})^2 + (\frac{4}{4})^2) = 1 - (0 + 1) = \mathbf{0}$

### Step 3: Calculate the Information Gain
- Calculate the weighted average Gini Impurity after the split.

$$Gini_{after\_split} = (\frac{6}{10}) \times 0.278 + (\frac{4}{10}) \times 0 = 0.1668 + 0 = 0.1668$$

- Finally, calculate the Information Gain.

$$\text{Information Gain} = Gini_{root} - Gini_{after\_split} = 0.5 - 0.1668 = \mathbf{0.3332}$$

- This value of 0.3332 represents the "goodness" of the split using the "Wind" feature. The decision tree performs similar calculations for other features (like Outlook, Humidity, etc.) and chooses the split rule that results in the highest Information Gain.
:::

## Why Is It Possible to Create an Infinite Number of Trees?

::: {.justify}
- Theoretically, it is possible to create an infinite number of decision trees. There are two main reasons for this:

  1.  **Perfectly Memorizing Data (Overfitting):** If no limits are placed on the tree's depth or splitting, the algorithm will **continue splitting until every single sample in the training data is isolated in its own leaf node**. This results in a model that is 100% accurate on the training data but is useless for predicting new, unseen data. This creates an overly complex tree known as an **overfit** model.
  
  2.  **Features with Continuous Values:** When features contain continuous values like "temperature" or "humidity," there are infinitely many potential split points (e.g., split at Temp > 25.0°C, Temp > 25.01°C, etc.). This theoretically allows for the creation of infinitely many different tree structures.

- In practice, to prevent such overfitting, techniques called **"Pruning"** are used to properly control the tree's complexity. This includes **limiting the maximum depth of the tree** or **setting a minimum number of samples required to be at a leaf node**.
:::

# Data Prep

::: {.justify}
- The fundamental prerequisite for building a supervised learning model is the use of **"labeled data."** This refers to a dataset where each data point is provided with a "correct answer" or label in advance. For example, this could be email data where each message is labeled as either "Spam" or "Not Spam." The model uses these correct labels as a target to learn patterns within the data.

- Furthermore, to fairly evaluate the model's performance on new, unseen data, the dataset is split into a **Training Set** and a **Testing Set**. This division prevents **"overfitting,"** where the model simply memorizes the training data, and allows for a proper measurement of its ability to generalize.

  - **Training Set**: The majority of the dataset, used for the model to learn the patterns and relationships in the data.
  - **Testing Set**: A portion of the data held back from the training process, used only to verify the final performance of the trained model.

- The training and testing data must be **disjoint** sets, meaning they have no data points in common. The reason is to get an honest evaluation of the model's ability to generalize to new, unseen data. If the sets overlap, the model is being tested on data it has already "seen" during training, which would be like giving a student an exam with questions they had already memorized. Keeping the training and testing sets **disjoint** is the only way to get a true measure of the model's predictive performance.

- The sample image of the data used in the analysis on this page is shown below. This dataset corresponds to the one shown in "Merged Cleaned Data" at the end of [Data Gathering and Data Cleaning](02_02_data_gathering_and_cleaning.qmd). The dataset can be found [here](https://github.com/taso5789/ds-project-portfolio/tree/main/data/df_merged.csv).
  - For this analysis, the target variables—inflation rates (`PCEPILFE_YoY`, `PCEPILFE_MoM`) and the unemployment rate (`UNRATE`)—were binarized to apply the Naïve Bayes classifier. The historical median of each variable was used as a threshold; periods with values above the median were defined as "High Inflation" or "High Unemployment" (1), and all others as "Low" (0).
:::

::: {.text-center}
![](results/5_Decision_Trees/data.jpg){width=100%}
:::

::: {.justify}
- The training and testing data used to predict the year-over-year (YoY) inflation (`PCEPILFE_YoY`), month-over-month (MoM) inflation (`PCEPILFE_MoM`), and unemployment rates (`UNRATE`) are shown in @fig-inf-YoY-train ~ @fig-unemp-test, respectively. Each dataset was created as follows:
:::

## Training and Test Data for YoY Inflation Prediction (`high_inflation_YoY` derived from `PCEPILFE_YoY`)

::: {.justify}
1.  **Feature Engineering**: To provide the model with historical context, the feature set was expanded. Specifically, **1, 2, and 3-month lag variables**, as well as **2- and 3-month rolling window statistics** (e.g., mean, standard deviation), were calculated for each economic indicator and added as features.

2.  **Defining Predictors and Target**:  The binarized YoY inflation rate (`high_inflation_YoY`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for YoY Inflation](results/5_Decision_Trees/inflation_YoY_train_data.jpg){#fig-inf-YoY-train}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for YoY Inflation](results/5_Decision_Trees/inflation_YoY_test_data.jpg){#fig-inf-YoY-test}
:::
:::
:::

## Training and Test Data for MoM Inflation Prediction (`high_inflation_MoM` derived from `PCEPILFE_MoM`)

::: {.justify}
1.  **Feature Engineering**: To provide the model with historical context, the feature set was expanded. Specifically, **1-month lag variables** and **2-month rolling window statistics** (e.g., mean, standard deviation) were calculated for each economic indicator and added as features.

2.  **Defining Predictors and Target**: The binarized MoM inflation rate (`high_inflation_MoM`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for MoM Inflation](results/5_Decision_Trees/inflation_MoM_train_data.jpg){#fig-inf-MoM-train fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for MoM Inflation](results/5_Decision_Trees/inflation_MoM_test_data.jpg){#fig-inf-MoM-test fig-align="center" width="100%"}
:::
:::
:::

## Training and Test Data for Unemployment Rates (`high_unemployment` derived from `UNRATE`)

::: {.justify}
1.  **Feature Engineering**: To provide the model with historical context, the feature set was expanded. Specifically, **8-month lag variables** and **3-month rolling window statistics** (e.g., mean, standard deviation) were calculated for each economic indicator and added as features.

2.  **Defining Predictors and Target**: The binarized unemployment rate (`high_unemployment`) was set as the target variable (y). All other potential target variables regarding inflation and unemployment were excluded from the feature set (X) to prevent data leakage.

3.  **Data Splitting**: The dataset was divided into a **70% training set** and a **30% testing set** using stratified sampling to ensure the class proportions were identical in both sets.

4.  **Selective Scaling**: **Only continuous variables were standardized** using `StandardScaler`, while dummy variables (e.g., for recessions) were excluded from this process. The scaler was fit on the training data and then applied to the test data to prevent data leakage.
:::

::: {.justify}
::: {.columns}
::: {.column width="47.5%"}
![The Training Data for Unemployment Rates](results/5_Decision_Trees/unemp_train_data.jpg){#fig-unemp-train fig-align="center" width="100%"}
:::
::: {.column width="5%"}
:::
::: {.column width="47.5%"}
![The Test Data for Unemployment Rates](results/5_Decision_Trees/unemp_test_data.jpg){#fig-unemp-test fig-align="center" width="100%"}
:::
:::
:::

# Code

::: {.justify}
- For the code used in this chapter (Naïve Bayes), please refer [here](https://github.com/taso5789/ds-project-portfolio/blob/main/code/5_Decision_Trees.ipynb).
:::

# Results

## Confusion Matrix for YoY Inflation Prediction

::: {.justify}
- **Accuracy**: **92.5%**

- This is an **exceptionally high-performing and well-balanced model**. Its primary strength is that both **precision and recall are above 90%** for both the "High Inflation" and "Low Inflation" classes.

- This indicates the model is simultaneously excellent at finding nearly all high inflation periods (94% recall) and ensuring that its predictions are highly reliable (91% precision). With only 5 total misclassifications, this Decision Tree model serves as an ideal and effective classifier for this task.
:::

::: {.justify}
![Confusion Matrix for YoY Inflation Prediction](results/5_Decision_Trees/inflation_YoY_confusion_matrix.jpg){#fig-inf-YoY-matrix fig-align="center" width="100%"}
:::

## Confusion Matrix for MoM Inflation Prediction

::: {.justify}
- **Accuracy**: **65.2%**

- This model's performance is **poor**, with an accuracy only slightly better than a random guess.

- Its primary weakness is **low precision** across both classes (e.g., 63% for "High Inflation"), which means its predictions are unreliable. When the model signals "High Inflation," it is incorrect more than a third of the time.

- Although it manages to find a majority of the high inflation periods (**71% recall**), it does so by generating a high number of false alarms. Overall, the model lacks the reliability required for practical use.
:::

::: {.justify}
![Confusion Matrix for MoM Inflation Prediction](results/5_Decision_Trees/inflation_MoM_confusion_matrix.jpg){#fig-inf-MoM-matrix fig-align="center" width="100%"}
:::

## Confusion Matrix for Unemployment Rates Prediction

::: {.justify}
- **Accuracy**: **97.0%**

-This is an **exceptionally high-performing and reliable model**, demonstrating near-perfect results.

-Its most notable strengths are two perfect metrics:
  1.  **100% precision** for "High Unemployment," which means its predictions of a high unemployment period are **never wrong**.
  2.  **100% recall** for "Low Unemployment," meaning it **never misclassifies** a low unemployment period.

- The model's only minor imperfection is missing 2 out of 32 high unemployment events, still resulting in a very strong **94% recall** for that class. Overall, this Decision Tree model shows nearly ideal performance for this task.
:::

::: {.justify}
![Confusion Matrix for Unemployment Rates Prediction](results/5_Decision_Trees/unemp_confusion_matrix.jpg){#fig-unemp-matrix fig-align="center" width="100%"}
:::

## Related to Q5 (out of 10 Research Questions) in [Introduction](01_introduction.qmd)

::: {.justify}
- Refer to the conclusion below.
:::

# Conclusions

::: {.justify}
- The Decision Tree model demonstrated markedly different performance depending on the prediction target, revealing that while it can be a powerful tool for identifying macroeconomic regimes, it also has clear limitations.

  1. **High Efficacy in Predicting Persistent States**: The model achieved extremely high accuracy when predicting indicators with relatively persistent trends, such as the **unemployment rate** and **year-over-year (YoY) inflation**. The unemployment prediction (97.0% accuracy), in particular, was nearly perfect in its reliability, showing that the Decision Tree is adept at capturing the patterns of stable economic states. The YoY inflation prediction (92.5% accuracy) also yielded high-performing and well-balanced results.
  
  2. **Challenges in Predicting Short-Term Fluctuations**: Conversely, for predicting noisy data with high short-term volatility, such as **month-over-month (MoM) inflation**, performance dropped significantly, resulting in unreliable results (65.2% accuracy). This suggests that the model struggles to distinguish between short-term random noise and the underlying signal.

- In summary, the Decision Tree model is **highly effective for the task of classifying persistent economic states** like unemployment and annual inflation but is **not well-suited for predicting volatile, short-term indicators** like month-to-month inflation rates. It was confirmed that the model's effectiveness is strongly dependent on the characteristics (e.g., stability, noise level) of the time-series data being predicted.
:::
