---
title: "Clustering"
---

::: {.justify}
Notes: All images displayed on this page are free-use materials, unless otherwise indicated by a specific credit.
:::

# Overview

::: {.justify}
- **Clustering** is an unsupervised learning technique that groups similar data points together based on their characteristics. The purpose of clustering is discoveryâ€”it allows us to identify hidden structures, patterns, or natural groupings within the data without relying on predefined labels. There are two major approaches to clustering: **partitional clustering** and **hierarchical clustering**.

  - **Partitional clustering** (e.g., k-means) divides the dataset into a fixed number of clusters. Each data point is assigned to exactly one cluster, and the algorithm iteratively updates cluster centers to minimize within-cluster distances. This approach is computationally efficient and works well when the number of clusters is known in advance.

  - **Hierarchical clustering** builds a tree-like structure (dendrogram) to represent nested groupings of the data. It can be performed in two ways: agglomerative (bottom-up, starting with individual points and merging them into clusters) or divisive (top-down, starting with one large cluster and splitting it into smaller clusters). Unlike k-means, it does not require specifying the number of clusters beforehand, and the dendrogram provides insight into multiple levels of grouping.

- A key element in clustering is the **distance metric**, which defines how similarity between data points is measured. Common metrics include Euclidean distance (straight-line distance in space), Manhattan distance (sum of absolute differences across dimensions), and Cosine similarity (measuring the angle between vectors, useful for text or high-dimensional data). The choice of distance metric can significantly affect the resulting clusters and must be carefully aligned with the nature of the data.

- In this project, clustering will be used to explore hidden groupings in the dataset and provide insights that may not be immediately visible through simple descriptive statistics. <mark><span style="color:red">How to apply clustering to my data</span></mark>
:::

::: {.columns}
::: {.column width="50%"}
![](images/fig_03_overview_01.png){fig-align="center" width="100%"}
:::
::: {.column width="50%"}
![](images/fig_03_overview_02.png){fig-align="center" width="100%"}
:::
:::

# Data Prep

::: {.justify}
- **Clustering methods require only unlabeled numeric data**. Unlike supervised learning, where models are trained with predefined labels or categories, clustering is an unsupervised approach that seeks to discover hidden groupings in the data without prior knowledge of class membership. Because of this, categorical labels or outcome variables are not necessary and, in fact, must be excluded during the clustering process to avoid biasing the results. In addition, the input data must be numeric since clustering algorithms, such as k-means and hierarchical clustering, rely on distance or similarity measures (e.g., Euclidean distance, cosine similarity). These measures are mathematically defined only for numerical variables. If the dataset includes categorical or textual features, they need to be transformed into numeric representations (for example, one-hot encoding for categorical variables or vectorization methods for text) before clustering can be applied.
:::

::: {.justify}
- The sample image of the data used in the analysis on this page is shown below. This dataset corresponds to the one shown in "Merged Cleaned Data" at the end of [Data Gathering and Data Cleaning](02_02_data_gathering_and_cleaning.qmd).
:::

::: {.text-center}
![](results/2_Clustering/data.jpg){width=90%}
:::

- <mark><span style="color:red">Link to Data</span></mark>


# Code

- <mark><span style="color:red">Link to Code</span></mark>

# Results

- xxx

# Conclusions

- xxx